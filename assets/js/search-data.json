{
  
    
        "post0": {
            "title": "Can I use a metric as a loss function?",
            "content": "What is a “loss” function? . Optimization algorithms try to optimize an objective function either by maximizing it or by minimizing it. . A loss function is by convention an objective function we wish to minimize. . “A loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some “cost” associated with the event. An optimization problem seeks to minimize a loss function.” (Wikipedia) . In other words, the loss function will return a smaller number for a better model. . What are metrics? . A metric is a function that is used to judge the performance of our model. . Some well known metrics for classification: . . Some well known metrics for regression: . . . Note: There are many more metrics for these problems and some for other problems. We don&#39;t try to enumerate them all here. Choosing metrics representative of the problem at hand and what matters isn’t always obvious. . Take the example of building a model that classifies whether Yes or No a patient with COVID-19 should be sent to intensive care, using supervised learning on a labeled dataset built from real world data. Miss rate (also called False negative rate) would be a good metric since you really want to the amount of times you predict No falsely to be minimal. Lives are at risk ! You may also want to take a look at your Fall-out (False positive rate) to avoid saturating the intensive care unit with people that don’t need it. . So what is the difference? . The loss function is the function your algorithm tries to minimize and the metric is what you evaluate your model on. You will always need a metric to evaluate your model but particular algorithms will rely on a separate loss function during training, often as a proxy. . Can I use the loss function as my metric? . Here we are talking about loss functions that actually work for your particular algorithm. As you will see later, not all metrics can be used as a loss function. . You can, except… . Sometimes, loss function values can be confusing and hard to interpret. . For example, if you use cross-entropy loss on a classification problem and get an output value of 2, you know it is better than 2.5 since you want to minimize it but how do you know if a loss of 2 or an improvement of 0.5 is actually any good? . That’s when metrics come into play. . Metrics are defined for human consumption and are what you care about when evaluating your model. . . Tip: You can use one or more metrics (even 0) for evaluating model performance. Otherwise it is fine . Sometimes your loss function can be used as a metric, meaning it is being minimized by the algorithm and humans can interpret its outputs easily. . An example would be Root-Mean-Square error. . Can I use my metric as the loss function? . Not always! . You can’t, for several reasons . Some algorithms require the loss function to be differentiable and some metrics, such as accuracy or any other step function, are not. . These algorithms usually use some form of gradient descent to update the parameters. This is the case for neural network weight stepping, where the partial derivative of the loss with respect to each weight is calculated. . Let’s illustrate this by using accuracy on a classification problem where the model assigns a probability to each mutually exclusive class for a given input. . In this case, a small change to a parameter’s weight may not change the outcome of our predictions but only our confidence in them, meaning the accuracy remains the same. The partial derivative of the loss with respect to this parameter would be 0 (infinity at the threshold) most of the time, preventing our model from learning (a step of 0 would keep the weight and model as is). . In other words, we want the small changes made to the parameter weights to be reflected in the loss function. . Some algorithms don’t require their function to be differentiable but would not work with some functions by their nature. You can read this post as an example of why classification error can’t be used for decision tree. . It may not be ideal . Some objective functions are easier to optimize than others. We might want to use a proxy easy loss function instead of a hard one. . We often choose to optimize smooth and convex loss functions because: . They are differentiable anywhere. | A minimum is always a global minimum. | . Using gradient descent on such function will lead you surely towards the global minima and not get stuck in a local mimimum or saddle point. . There are plenty of ressources about convex functions on the internet. I’ll share one with you. I personally didn’t get all of it but maybe you will. . Some algorithms seem to empirically work well with non-convex functions. This is the case of Deep Learning for example, where we often use gradient descent on a non-convex loss function. . . Note: The stochasticity introduced by the use of minibatch, random initialization or momentum methods help avoid saddle points and local minimas to some extent, but it also seems like increasing network size increases the chance of finding a local minimum with good generalization. Refer to this (or to the first part of the 4th chapter of *Fundamentals of Deep Learning by Nikhil Buduma* for a more intuitive answer). Another thing you need to be careful of is that different loss functions bring different assumptions to the model. For example, the logistic regression loss assumes a Bernoulli distribution. . Sure go ahead . If your metric doesn’t fall into the limitations mentioned above then go ahead ! Optimizing directly on what matters (the metric) is ideal. . Is it clearer now ? . Did I help clear things up or is it even more confusing now? Or am I mistaken somehow? You can share your questions / remarks regarding the subject by writing a comment or annotating the text using Hypothes.is! .",
            "url": "https://jonathan-sands.com/metric/loss/2021/05/13/Metric-vs-Loss.html",
            "relUrl": "/metric/loss/2021/05/13/Metric-vs-Loss.html",
            "date": " • May 13, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Drug consumption dataset data analysis",
            "content": "This post is a great way to dive in data analysis and get familiar with python tools such as pandas, seaborn, matplotlib, numpy. . This is also an opportunity to work on multilabel data (several target variables that are not mutually exclusive). . Setting up workspace . import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.ticker as ticker import seaborn as sns import matplotlib as mpl import math from scipy.stats import pearsonr, spearmanr, chi2_contingency, ttest_ind, mannwhitneyu, norm, normaltest, shapiro, anderson import operator from IPython.display import HTML, display import seaborn as sns . sns.set(rc={&#39;figure.figsize&#39;:(10,5)}) sns.set_style(&#39;whitegrid&#39;) %matplotlib inline . Dataset preparation . Drug consumption (quantified) Data Set is a multilabel dataset from the UCI machine learning repository. . The dataset contains records for 1885 respondents. . To summarize, it contains: . an ID column | 5 demographic columns (features) | 7 personality traits (features) | 18 drugs with their usage frequency (target) | a fake drug called Semeron to verify reliability of answers | . Each drug variable can take 6 different values: . CL0 Never Used | CL1 Used over a Decade | CL2 Used in the Last Decade | CL3 Used in the Last Year | CL4 Used in the Last Month | CL5 Used in the Last Week | CL6 Used in the Last Day | . All qualitative variables where transformed to real values (except for the target variables). . Tip: Read the dataset description for more details! . The following lists will be useful for loading and indexing the data. . demographic_columns = [ &#39;Age&#39;, &#39;Gender&#39;, &#39;Education&#39;, &#39;Country&#39;, &#39;Ethnicity&#39;, ] personality_columns = [ &#39;Neuroticism&#39;, &#39;Extraversion&#39;, &#39;Openness to experience&#39;, &#39;Agreeableness&#39;, &#39;Conscientiousness&#39;, &#39;Impulsiveness&#39;, &#39;Sensation seeking&#39; ] feature_columns = demographic_columns + personality_columns drugs_columns = [ &#39;Alcohol consumption&#39;, &#39;Amphetamines consumption&#39;, &#39;Amyl nitrite consumption&#39;, &#39;Benzodiazepine consumption&#39;, &#39;Caffeine consumption&#39;, &#39;Cannabis consumption&#39;, &#39;Chocolate consumption&#39;, &#39;Cocaine consumption&#39;, &#39;Crack consumption&#39;, &#39;Ecstasy consumption&#39;, &#39;Heroin consumption&#39;, &#39;Ketamine consumption&#39;, &#39;Legal highs consumption&#39;, &#39;Lysergic acid diethylamide consumption&#39;, &#39;Methadone consumption&#39;, &#39;Magic mushrooms consumption&#39;, &#39;Nicotine consumption&#39;, &#39;Fictitious drug Semeron consumption&#39;, &#39;Volatile substance abuse consumption&#39; ] drugs_legal = [&#39;Alcohol consumption&#39;, &#39;Caffeine consumption&#39;, &#39;Chocolate consumption&#39;, &#39;Nicotine consumption&#39;] drugs_illegal = [drug for drug in drugs_columns if drug not in drugs_legal] all_columns = feature_columns + drugs_columns . Loading the dataset . dataset = pd.read_csv(&quot;drug_consumption.data&quot;, names=[&quot;ID&quot;] + all_columns) . dataset = dataset.set_index(&quot;ID&quot;) . dataset.head() . Age Gender Education Country Ethnicity Neuroticism Extraversion Openness to experience Agreeableness Conscientiousness ... Ecstasy consumption Heroin consumption Ketamine consumption Legal highs consumption Lysergic acid diethylamide consumption Methadone consumption Magic mushrooms consumption Nicotine consumption Fictitious drug Semeron consumption Volatile substance abuse consumption . ID . 1 0.49788 | 0.48246 | -0.05921 | 0.96082 | 0.12600 | 0.31287 | -0.57545 | -0.58331 | -0.91699 | -0.00665 | ... | CL0 | CL0 | CL0 | CL0 | CL0 | CL0 | CL0 | CL2 | CL0 | CL0 | . 2 -0.07854 | -0.48246 | 1.98437 | 0.96082 | -0.31685 | -0.67825 | 1.93886 | 1.43533 | 0.76096 | -0.14277 | ... | CL4 | CL0 | CL2 | CL0 | CL2 | CL3 | CL0 | CL4 | CL0 | CL0 | . 3 0.49788 | -0.48246 | -0.05921 | 0.96082 | -0.31685 | -0.46725 | 0.80523 | -0.84732 | -1.62090 | -1.01450 | ... | CL0 | CL0 | CL0 | CL0 | CL0 | CL0 | CL1 | CL0 | CL0 | CL0 | . 4 -0.95197 | 0.48246 | 1.16365 | 0.96082 | -0.31685 | -0.14882 | -0.80615 | -0.01928 | 0.59042 | 0.58489 | ... | CL0 | CL0 | CL2 | CL0 | CL0 | CL0 | CL0 | CL2 | CL0 | CL0 | . 5 0.49788 | 0.48246 | 1.98437 | 0.96082 | -0.31685 | 0.73545 | -1.63340 | -0.45174 | -0.30172 | 1.30612 | ... | CL1 | CL0 | CL0 | CL1 | CL0 | CL0 | CL2 | CL2 | CL0 | CL0 | . 5 rows × 31 columns . We successfully loaded the dataset and set the ID as row index. . dataset.dtypes . Age float64 Gender float64 Education float64 Country float64 Ethnicity float64 Neuroticism float64 Extraversion float64 Openness to experience float64 Agreeableness float64 Conscientiousness float64 Impulsiveness float64 Sensation seeking float64 Alcohol consumption object Amphetamines consumption object Amyl nitrite consumption object Benzodiazepine consumption object Caffeine consumption object Cannabis consumption object Chocolate consumption object Cocaine consumption object Crack consumption object Ecstasy consumption object Heroin consumption object Ketamine consumption object Legal highs consumption object Lysergic acid diethylamide consumption object Methadone consumption object Magic mushrooms consumption object Nicotine consumption object Fictitious drug Semeron consumption object Volatile substance abuse consumption object dtype: object . Features are all floats and targets are objects. . dataset.shape . (1885, 31) . 1885 observations and 31 variables. . dataset.isnull().values.any() . False . There are no missing values. . Nominal drug to ordinal data . for i in drugs_columns: dataset[i] = dataset[i].map({&#39;CL0&#39;: 0, &#39;CL1&#39;: 1, &#39;CL2&#39;: 2, &#39;CL3&#39;: 3, &#39;CL4&#39;: 4, &#39;CL5&#39;: 5, &#39;CL6&#39;: 6}) . Mapping CLX to X. Now all variables are numeric. . Removing Semeron users . Let&#39;s see who claimed to took Semeron. . semerons = dataset[dataset[&#39;Fictitious drug Semeron consumption&#39;] != 0] semerons . Age Gender Education Country Ethnicity Neuroticism Extraversion Openness to experience Agreeableness Conscientiousness ... Ecstasy consumption Heroin consumption Ketamine consumption Legal highs consumption Lysergic acid diethylamide consumption Methadone consumption Magic mushrooms consumption Nicotine consumption Fictitious drug Semeron consumption Volatile substance abuse consumption . ID . 730 -0.07854 | 0.48246 | -1.73790 | -0.09765 | -0.31685 | -0.58016 | 0.32197 | 0.14143 | -0.60633 | 0.12331 | ... | 2 | 2 | 2 | 0 | 4 | 2 | 6 | 6 | 2 | 2 | . 821 -0.95197 | -0.48246 | -0.61113 | -0.09765 | -0.50212 | -0.67825 | 1.74091 | 0.72330 | 0.13136 | 0.41594 | ... | 3 | 0 | 0 | 0 | 5 | 0 | 5 | 4 | 3 | 0 | . 1520 -0.95197 | -0.48246 | -0.61113 | -0.57009 | -0.31685 | -0.24649 | -0.80615 | -1.27553 | -1.34289 | -1.92173 | ... | 1 | 2 | 1 | 2 | 1 | 2 | 4 | 2 | 3 | 1 | . 1537 -0.95197 | 0.48246 | -0.61113 | -0.57009 | 0.11440 | -0.46725 | 0.80523 | 0.29338 | 2.03972 | 1.81175 | ... | 4 | 0 | 4 | 3 | 2 | 0 | 3 | 4 | 4 | 3 | . 1702 0.49788 | 0.48246 | 0.45468 | -0.57009 | -0.31685 | 1.98437 | -0.80615 | 2.15324 | 0.76096 | -0.00665 | ... | 2 | 0 | 2 | 2 | 2 | 0 | 2 | 6 | 2 | 0 | . 1773 -0.95197 | -0.48246 | -1.22751 | -0.57009 | -0.22166 | -0.34799 | 1.28610 | 1.06238 | -0.01729 | -0.52745 | ... | 3 | 0 | 4 | 3 | 6 | 3 | 3 | 3 | 1 | 3 | . 1810 -0.95197 | 0.48246 | -1.43719 | -0.57009 | -0.31685 | 1.23461 | 1.11406 | 1.06238 | -1.47955 | 0.12331 | ... | 4 | 2 | 1 | 4 | 1 | 0 | 1 | 6 | 1 | 2 | . 1827 -0.95197 | 0.48246 | 0.45468 | -0.57009 | -0.31685 | 0.22393 | -0.30033 | 0.88309 | 1.28610 | -0.00665 | ... | 0 | 0 | 0 | 2 | 3 | 0 | 3 | 5 | 2 | 0 | . 8 rows × 31 columns . There are 8 people that lied about taking Semeron, which is a fake drug, so we should remove them. . dataset = dataset[dataset[&#39;Fictitious drug Semeron consumption&#39;] == 0] # Removing it from drug lists drugs_columns.remove(&#39;Fictitious drug Semeron consumption&#39;) drugs_illegal.remove(&#39;Fictitious drug Semeron consumption&#39;) #Dropping the column from the dataset dataset.drop(columns=&#39;Fictitious drug Semeron consumption&#39;) dataset.shape . (1877, 31) . We have 1877 respondents left. . Tip: If you ever acquire data from individuals, try a similar approach to get rid of untrustworthy observations. . Exploratory Data analysis . dataset.describe() . Age Gender Education Country Ethnicity Neuroticism Extraversion Openness to experience Agreeableness Conscientiousness ... Ecstasy consumption Heroin consumption Ketamine consumption Legal highs consumption Lysergic acid diethylamide consumption Methadone consumption Magic mushrooms consumption Nicotine consumption Fictitious drug Semeron consumption Volatile substance abuse consumption . count 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | ... | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.0 | 1877.000000 | . mean 0.037577 | -0.000771 | -0.000984 | 0.358984 | -0.309728 | -0.000551 | -0.001951 | -0.003224 | -0.000657 | -0.000394 | ... | 1.309536 | 0.372403 | 0.564198 | 1.353223 | 1.053277 | 0.826319 | 1.177944 | 3.194992 | 0.0 | 0.429409 | . std 0.878387 | 0.482588 | 0.949831 | 0.699707 | 0.166220 | 0.998442 | 0.997418 | 0.995691 | 0.996689 | 0.997657 | ... | 1.647373 | 1.034761 | 1.216341 | 1.790881 | 1.484582 | 1.648379 | 1.459212 | 2.415814 | 0.0 | 0.959160 | . min -0.951970 | -0.482460 | -2.435910 | -0.570090 | -1.107020 | -3.464360 | -3.273930 | -3.273930 | -3.464360 | -3.464360 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.000000 | . 25% -0.951970 | -0.482460 | -0.611130 | -0.570090 | -0.316850 | -0.678250 | -0.695090 | -0.717270 | -0.606330 | -0.652530 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 0.0 | 0.000000 | . 50% -0.078540 | -0.482460 | -0.059210 | 0.960820 | -0.316850 | 0.042570 | 0.003320 | -0.019280 | -0.017290 | -0.006650 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 3.000000 | 0.0 | 0.000000 | . 75% 0.497880 | 0.482460 | 0.454680 | 0.960820 | -0.316850 | 0.629670 | 0.637790 | 0.723300 | 0.760960 | 0.584890 | ... | 3.000000 | 0.000000 | 0.000000 | 3.000000 | 2.000000 | 0.000000 | 2.000000 | 6.000000 | 0.0 | 0.000000 | . max 2.591710 | 0.482460 | 1.984370 | 0.960820 | 1.907250 | 3.273930 | 3.273930 | 2.901610 | 3.464360 | 3.464360 | ... | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 0.0 | 6.000000 | . 8 rows × 31 columns . The representation above is quite archaic. Let&#39;s create a plot to make it easier to read. . fig, ax = plt.subplots(figsize=(11,15)) plt.ylabel(&quot;Features&quot;) plt.title(&quot;Box plot of Pre-Processed Data Set&quot;) ax = sns.boxplot(data = dataset[feature_columns], orient=&quot;h&quot;, palette=&quot;Set2&quot;) sns.reset_orig() . Seems like the dataset creators centered the features near 0 and scaled them with a small std (around 1), so we can probably keep the current encoding to build a model. . Demographic data analysis . Rearranging data . For data analysis, we will transform the quantified categorical data back to a clearer, nominal, form. . . Note: Our mapping is based on the dataset description . demo_data = dataset.copy() . age = [&#39;18-24&#39; if a &lt;= -0.9 else &#39;25-34&#39; if a &gt;= -0.5 and a &lt; 0 else &#39;35-44&#39; if a &gt; 0 and a &lt; 1 else &#39;45-54&#39; if a &gt; 1 and a &lt; 1.5 else &#39;55-64&#39; if a &gt; 1.5 and a &lt; 2 else &#39;65+&#39; for a in demo_data[&#39;Age&#39;]] gender = [&#39;Female&#39; if g &gt; 0 else &quot;Male&quot; for g in demo_data[&#39;Gender&#39;]] education = [&#39;Left school before 16 years&#39; if e &lt;-2 else &#39;Left school at 16 years&#39; if e &gt; -2 and e &lt; -1.5 else &#39;Left school at 17 years&#39; if e &gt; -1.5 and e &lt; -1.4 else &#39;Left school at 18 years&#39; if e &gt; -1.4 and e &lt; -1 else &#39;Some college or university, no certificate or degree&#39; if e &gt; -1 and e &lt; -0.5 else &#39;Professional certificate/ diploma&#39; if e &gt; -0.5 and e &lt; 0 else &#39;University degree&#39; if e &gt; 0 and e &lt; 0.5 else &#39;Masters degree&#39; if e &gt; 0.5 and e &lt; 1.5 else &#39;Doctorate degree&#39; for e in demo_data[&#39;Education&#39;]] country = [&#39;USA&#39; if c &lt; -0.5 else &#39;New Zealand&#39; if c &gt; -0.5 and c &lt; -0.4 else &#39;Other&#39; if c &gt; -0.4 and c &lt; -0.2 else &#39;Australia&#39; if c &gt; -0.2 and c &lt; 0 else &#39;Ireland&#39; if c &gt; 0 and c &lt; 0.23 else &#39;Canada&#39; if c &gt; 0.23 and c &lt; 0.9 else &#39;UK&#39; for c in demo_data[&#39;Country&#39;]] ethnicity = [&#39;Black&#39; if e &lt; -1 else &#39;Asian&#39; if e &gt; -1 and e &lt; -0.4 else &#39;White&#39; if e &gt; -0.4 and e &lt; -0.25 else &#39;Mixed-White/Black&#39; if e &gt;= -0.25 and e &lt; 0.11 else &#39;Mixed-White/Asian&#39; if e &gt; 0.12 and e &lt; 1 else &#39;Mixed-Black/Asian&#39; if e &gt; 1.9 else &#39;Other&#39; for e in demo_data[&#39;Ethnicity&#39;]] demo_data[&#39;Age&#39;] = age demo_data[&#39;Gender&#39;] = gender demo_data[&#39;Education&#39;] = education demo_data[&#39;Country&#39;] = country demo_data[&#39;Ethnicity&#39;] = ethnicity . demo_data[demographic_columns].head() . Age Gender Education Country Ethnicity . ID . 1 35-44 | Female | Professional certificate/ diploma | UK | Mixed-White/Asian | . 2 25-34 | Male | Doctorate degree | UK | White | . 3 35-44 | Male | Professional certificate/ diploma | UK | White | . 4 18-24 | Female | Masters degree | UK | White | . 5 35-44 | Female | Doctorate degree | UK | White | . This looks better :grin: . Proportion analysis . Let&#39;s start of by taking a look at the demographic data balance. . But first, we will define a few useful functions that will be reused throughout this analysis. . def plot_density(dataset, col, ax, plot_gaussian = True, color=&quot;Blue&quot;): &#39;&#39;&#39; Extension of the seaborn histogram that plots, for a given column, an estimated normal distribution (if requested) on top of the fitted data distribution. &#39;&#39;&#39; ncount = len(dataset) if plot_gaussian: std = dataset[col].std() mean = dataset[col].mean() #plot histogram using seaborn ax = sns.histplot(dataset[col], ax=ax, color=color, kde=True, stat=&quot;probability&quot;, kde_kws={&quot;bw_adjust&quot;:3}) if plot_gaussian: # Limiting our gaussian to the limits from the above plot xmin, xmax = ax.get_xlim() xnorm = np.arange(xmin, xmax, 0.001) ynorm = norm.pdf(xnorm, mean, std) ax.plot(xnorm, ynorm, &#39;r&#39;, lw=2) ax.legend([&quot;data distribution&quot;, &quot;estimated normal distribution&quot;], loc=&quot;upper center&quot;, bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=2) ax.set_title(col) ax.set_xlabel(&quot;&quot;) . def plot_pie(dataset, col, ax): &#39;&#39;&#39; Pandas&#39; pie plot wrapper &#39;&#39;&#39; ax = dataset[col].value_counts().plot(kind=&#39;pie&#39;, ax=ax) ax.set_title(col) ax.set_ylabel(&quot;&quot;) . def plot_count(dataset, col, ax, order = None, show_percent=True, rotate_label = True, add_args={&quot;palette&quot;: &quot;Set2&quot;}): &#39;&#39;&#39; Extending the seaorn countplot to get frequencies and counts in a pretty way. &#39;&#39;&#39; ncount = len(dataset) if order is None: order = np.sort(dataset[col].unique()) # plot seaborn countplot ax = sns.countplot(data=dataset, x=col, ax=ax, order=order, **add_args) # Get y limit (number of elements) _ ,max_nb = ax.get_ylim() # Transform this limit into a frequency in [0, 100] freq_lim = (max_nb * 100/ ncount) # Duplicate the ax ax2 = ax.twinx() #Move duplicate y axis ticks to the left ax2.yaxis.tick_left() #Move original y axis ticks to the right ax.yaxis.tick_right() # Swap the label positions to match the ticks ax.yaxis.set_label_position(&#39;right&#39;) ax2.yaxis.set_label_position(&#39;left&#39;) ax2.set_ylabel(&#39;Frequency [%]&#39;) # We want to write the frequency on top of each bar if show_percent: # for every bar for p in ax.patches: x=p.get_bbox().get_points()[:,0] y=p.get_bbox().get_points()[1,1] if not math.isnan(x.mean()) and not math.isnan(y): # Write frequency at an x and y coordinate ax.text(x.mean(), y, &#39;{:.1f}%&#39;.format(100.*y/ncount), ha=&#39;center&#39;, va=&#39;bottom&#39;) # Set y axis limit for counts and frequencies ax2.set_ylim(0,freq_lim) ax.set_ylim(0,max_nb) # set ticks for count ax.yaxis.set_major_locator(ticker.LinearLocator(11)) ax.yaxis.set_tick_params(which=&quot;major&quot;) # set ticks for frequencies ax2.yaxis.set_major_locator(ticker.MultipleLocator(freq_lim/10)) ax2.yaxis.set_tick_params(which=&quot;major&quot;) # remove grid for ax 2 (keep only ax) ax2.grid(False) ax.grid(False) ax.set_xlabel(&quot;&quot;) if rotate_label: # rotate tick labels on the x axis / / / _ = ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=&quot;right&quot;) ax.set_title(col) . def plot(kind, dataset, columns=None, fig_title=&quot;Count/Frequency plots&quot;, fontsizes = 20, **kwargs): &#39;&#39;&#39; Wrapper function that takes care of plot wise sizes and calling the wanted procedure &#39;&#39;&#39; # plot choices kind_dict = { &#39;pie&#39;: plot_pie, &#39;count&#39;: plot_count, &#39;density&#39;: plot_density} if kind not in kind_dict: raise ValueError(f&quot;{kind} is not a valid kind, has to be one of {kind_dict.keys()}&quot;) if columns is None: # us all dataset columns columns = list(dataset.columns) fig = None # Setting font sizes plt.rc(&#39;xtick&#39;, labelsize=fontsizes*1.5) plt.rc(&#39;ytick&#39;, labelsize=fontsizes*1.5) plt.rc(&#39;axes&#39;, labelsize=fontsizes*2) plt.rc(&#39;legend&#39;, fontsize=fontsizes*1.5, title_fontsize=0) plt.rc(&#39;axes&#39;, titlesize=2*fontsizes) plt.rc(&#39;font&#39;, size=1.7*fontsizes) # Scale of the figure in ax (to be used later) figsize_scale = fontsizes if not isinstance(columns, list): # columns has to be a list if isinstance(columns, str): columns = [columns] else: columns = list(columns) if len(columns) == 1: # Only variable to plot ncols, nrows = 1, 1 figsize_scale *= 2 # double figsize else: ncols, nrows = 2, math.ceil(len(columns) / 2) fig, axes = plt.subplots(figsize=(figsize_scale*ncols, figsize_scale*nrows), nrows=nrows, ncols=ncols) if ncols == 1 and nrows == 1: # We need a list of axes axes = np.array([axes]) # Plot do_plots(dataset, columns, axes, kind_dict[kind], **kwargs) fig.suptitle(fig_title + &quot; n n&quot;, fontsize=fontsizes*2.5) plt.tight_layout() #Reset plot setting to original sns.reset_orig() def do_plots(dataset, columns, axes, plot_func, **kwargs): &#39;&#39;&#39; Calls the plotting function on every axis and removes unused axes. &#39;&#39;&#39; axes = axes.flat #plot every variable for index, col in enumerate(columns): plot_func(dataset, col, axes[index], **kwargs) # remove empty axes for empty in range(len(columns), len(axes)): axes[empty].axis(&quot;off&quot;) . . Important: If you are new to using python and the various libraries for data analysis, try to read through the above functions and understand what they do. Let&#39;s create pie plots and count plots. . plot(&quot;pie&quot;, demo_data, demographic_columns, fig_title=&quot;Plot pies of demographic data&quot;) . plot(&quot;count&quot;, demo_data, demographic_columns, fig_title=&quot;Count / Frequency plots of demographic data&quot;) . We can see some features are pretty balanced but in some cases, classes are underrepresented. We will go over each of them in more detail. . def value_counts_percentage(dataset, column): &#39;&#39;&#39; value.counts() method extended by displaying percentage &#39;&#39;&#39; a = dataset[column].value_counts() b = dataset[column].value_counts(normalize=True) * 100 return pd.concat([a,b.round(2)], axis=1, keys=[&#39;N&#39;, &#39;%&#39;]) . . Note: The above function creates a dataframe with count and frequencies for a given column. . Age . value_counts_percentage(demo_data, &#39;Age&#39;) . N % . 18-24 637 | 33.94 | . 25-34 480 | 25.57 | . 35-44 355 | 18.91 | . 45-54 294 | 15.66 | . 55-64 93 | 4.95 | . 65+ 18 | 0.96 | . The age groups are intervals of around 10 years. As we can see, the older the age group, the less represented they are. . The 18-24 group holds around 1/3 of all respondents ! . Unbalanced and not representative. . Gender . value_counts_percentage(demo_data, &#39;Gender&#39;) . N % . Male 940 | 50.08 | . Female 937 | 49.92 | . Gender is balanced and representative. . Education . value_counts_percentage(demo_data, &#39;Education&#39;) . N % . Some college or university, no certificate or degree 503 | 26.80 | . University degree 478 | 25.47 | . Masters degree 283 | 15.08 | . Professional certificate/ diploma 270 | 14.38 | . Left school at 18 years 99 | 5.27 | . Left school at 16 years 98 | 5.22 | . Doctorate degree 89 | 4.74 | . Left school at 17 years 29 | 1.55 | . Left school before 16 years 28 | 1.49 | . There is a predomination of educated people (around 85% are of college or above level), causing an unbalance. . Not sure these are representative of the real life distribution, but looking at the educational attainment in the United States, it seems the proportion are not so far off for developped countries (and they are very represented in this dataset). . Warning: That doesn&#8217;t mean our data is balanced, but by taking people randomnly we would get around the same distribution. . . Tip: A representative dataset is useful to build a representative model that describes the data rather than predicting. On the other hand, if your objective is to build a a predictive model, particularly one that performs well by measure of AUC or rank-order and plan to use a basic ML framework, then feeding it a balanced dataset is recommended. You can also use a representative dataset in the latter case and use custom sampling that will take care of balancing the data for training. . Country . value_counts_percentage(demo_data, &#39;Country&#39;) . N % . UK 1044 | 55.62 | . USA 551 | 29.36 | . Other 118 | 6.29 | . Canada 87 | 4.64 | . Australia 52 | 2.77 | . Ireland 20 | 1.07 | . New Zealand 5 | 0.27 | . All the known countries (90+%) are English speaking with US and UK totalling to over 80% of the data. . This is a clear unbalance and isn&#39;t representative. . Ethnicity . value_counts_percentage(demo_data, &#39;Ethnicity&#39;) . N % . White 1715 | 91.37 | . Other 62 | 3.30 | . Black 33 | 1.76 | . Asian 25 | 1.33 | . Mixed-White/Asian 20 | 1.07 | . Mixed-White/Black 19 | 1.01 | . Mixed-Black/Asian 3 | 0.16 | . 90+% of respondents are white. Again, a clear unbalance and is not representative. . Age-Gender . plot(&quot;count&quot;, demo_data, &#39;Age&#39;, fig_title=&quot;Age-Gender count/frequency&quot;, rotate_label=False, add_args={&quot;hue&quot;:&quot;Gender&quot;, &quot;palette&quot;:&#39;ch:.25&#39;}, fontsizes=4.5); . There are clearly a lot more men in the 18-24 age goupe. | More women in the 25-34 age group. | As the age increases, the gender proportion gradually evens out. | . Country-Gender . plot(&quot;count&quot;, demo_data, &#39;Country&#39;, fig_title=&quot;Age-Gender count/frequency&quot;, rotate_label=False, add_args={&quot;hue&quot;:&quot;Gender&quot;, &quot;palette&quot;:&#39;ch:.25&#39;}, fontsizes=4.5); . More women than men were tested in the USA, New-Zealand (see next table) and &quot;Other&quot;. | More men than women in the UK. | Around the same in the remaining countries. | . . Note: Very few respondents in some countries meaning we can&#8217;t tell if the respondents were taken from the real life distribution or not. . Age &#8211; Gender &#8211; Country cross table . pd.crosstab(demo_data[&#39;Age&#39;], [demo_data[&#39;Gender&#39;], demo_data[&#39;Country&#39;]]) . Gender Female Male . Country Australia Canada Ireland New Zealand Other UK USA Australia Canada Ireland New Zealand Other UK USA . Age . 18-24 1 | 7 | 3 | 0 | 12 | 112 | 99 | 20 | 25 | 5 | 2 | 39 | 91 | 221 | . 25-34 10 | 9 | 2 | 1 | 13 | 200 | 58 | 5 | 8 | 3 | 2 | 26 | 80 | 63 | . 35-44 4 | 8 | 3 | 0 | 7 | 150 | 29 | 3 | 7 | 0 | 0 | 10 | 108 | 26 | . 45-54 5 | 7 | 1 | 0 | 4 | 127 | 14 | 3 | 4 | 3 | 0 | 5 | 99 | 22 | . 55-64 0 | 7 | 0 | 0 | 0 | 29 | 7 | 1 | 1 | 0 | 0 | 2 | 36 | 10 | . 65+ 0 | 3 | 0 | 0 | 0 | 5 | 0 | 0 | 1 | 0 | 0 | 0 | 7 | 2 | . New-Zealand doesn&#39;t have anyone over 34 years old and tested only one woman. . | Ireland doesn&#39;t have anyone over 54 and has no man in the 35-44 age group. . | Australia doesn&#39;t have anyone over 65 (and only one over 54) and 20 times more males in the 18-24 age group. . | Canada has 3 times more men in the 18-24 group and has only 2 males above 54. . | UK has over twice more women than men in the 25-34 group. . | USA has over twice more men in the 18-24 group and no woman over 64. . | Other countries have 3 times more men for the 18-24 age group, twice for the 25-34 group, doesn&#39;t have any woman in the 55-64 group and doesn&#39;t have anyone over 64. . | . def catplot(data, x, hue, col, rotate_label=True, palette=&quot;ch:.25&quot;): plot = sns.catplot(x=x, hue=hue, col=col, palette=palette, data=data.sort_values(by=[x]), kind=&quot;count&quot;) if rotate_label: for axes in plot.axes.flat: _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=40, ha=&quot;right&quot;) . catplot(demo_data, &#39;Country&#39;, &#39;Age&#39;, &#39;Gender&#39;) . Using the above plot, some aspects described above pop up quite nicely. . Education &#8211; Gender &#8211; Ethnicity cross table . demo_data.pivot_table(index=&#39;Education&#39;, columns=[&#39;Gender&#39;, &#39;Ethnicity&#39;], aggfunc=&#39;size&#39;, fill_value=0) . Gender Female Male . Ethnicity Asian Black Mixed-Black/Asian Mixed-White/Asian Mixed-White/Black Other White Asian Black Mixed-Black/Asian Mixed-White/Asian Mixed-White/Black Other White . Education . Doctorate degree 1 | 0 | 0 | 1 | 2 | 1 | 52 | 1 | 0 | 0 | 0 | 0 | 2 | 29 | . Left school at 16 years 0 | 0 | 0 | 0 | 0 | 2 | 40 | 0 | 0 | 0 | 1 | 1 | 0 | 54 | . Left school at 17 years 0 | 0 | 0 | 0 | 1 | 0 | 12 | 0 | 0 | 0 | 0 | 0 | 2 | 14 | . Left school at 18 years 0 | 0 | 0 | 0 | 0 | 2 | 35 | 0 | 1 | 0 | 1 | 2 | 2 | 56 | . Left school before 16 years 0 | 0 | 0 | 0 | 0 | 0 | 12 | 0 | 0 | 0 | 0 | 0 | 0 | 16 | . Masters degree 4 | 6 | 0 | 3 | 5 | 5 | 156 | 5 | 1 | 0 | 0 | 1 | 4 | 93 | . Professional certificate/ diploma 0 | 3 | 0 | 1 | 0 | 2 | 130 | 1 | 5 | 0 | 1 | 1 | 6 | 120 | . Some college or university, no certificate or degree 3 | 2 | 1 | 2 | 1 | 8 | 153 | 0 | 4 | 1 | 4 | 3 | 12 | 309 | . University degree 7 | 6 | 1 | 4 | 2 | 8 | 263 | 3 | 5 | 0 | 2 | 0 | 6 | 171 | . Basically few entries of people leaving school at or before 18. . | Non-white: no one left school before being 16 and appart for the &#39;Other&#39; category, there is only one female that didn&#39;t get a higher education. No male** expect in &#39;Other&#39; have a Doctorate degree. . | Asian: no respondent with lower education and *no male in some college or university (but has some with masters degree, so doesn&#39;t mean no one went through university etc.) . | Black: only one person with lower education (one male left school at 18) and has no female with a professional diploma when it is the field with most respondents on the male side. . | Mixed-Black/Asian: very few respondents and only people following some college or university, or with an university degree are represented. . | Mixed-White/Asian: all the female respondents have high education and 2 males left school at 18 or before. Males, unlike females have no master&#39;s degree respondent. . | Mixed-White/Black: all females except 2 have a finished university, master&#39;s or doctorate degree, while males have only one master&#39;s degree and the other have either unfinished university degrees, a professional diploma or left school at 18 or before. . | Other: lots of males have professional diplomas, while no female has one. There is a high number of doctorate degrees but also a high number of respondents that left school at 18 or before (both relative to the categories up until now). . | White: basically all the respondents are white as we saw before. Both genders have respondents in all categories but not in the same proportions. To see things clearer, you can refer to the table below (we will focus on proportion instead of counts). The majority of males are pursuing a college or university degree, while the majority of females have an university degree. Nearly twice as many females have a doctorate degree and more male have left school at 18 or before. . | . pd.concat([value_counts_percentage(demo_data[(demo_data[&#39;Ethnicity&#39;] == &#39;White&#39;) &amp; (demo_data[&#39;Gender&#39;] == &#39;Male&#39;)], &#39;Education&#39;), value_counts_percentage(demo_data[(demo_data[&#39;Ethnicity&#39;] == &#39;White&#39;) &amp; (demo_data[&#39;Gender&#39;] == &#39;Female&#39;)], &#39;Education&#39;)], axis = 1, keys=[&#39;Male&#39;, &#39;Female&#39;]) . Male Female . N % N % . Some college or university, no certificate or degree 309 | 35.85 | 153 | 17.94 | . University degree 171 | 19.84 | 263 | 30.83 | . Professional certificate/ diploma 120 | 13.92 | 130 | 15.24 | . Masters degree 93 | 10.79 | 156 | 18.29 | . Left school at 18 years 56 | 6.50 | 35 | 4.10 | . Left school at 16 years 54 | 6.26 | 40 | 4.69 | . Doctorate degree 29 | 3.36 | 52 | 6.10 | . Left school before 16 years 16 | 1.86 | 12 | 1.41 | . Left school at 17 years 14 | 1.62 | 12 | 1.41 | . . Note: Beause of the huge unbalance in ethnicity, plots will be hard to interpret. See for yourself below. Changing the axis and hue won&#8217;t solve anything. . catplot(demo_data, &#39;Ethnicity&#39;, &#39;Education&#39;, &#39;Gender&#39;) . Country &#8211; Gender &#8211; Ethnicity cross table . pd.crosstab(demo_data[&#39;Country&#39;], [demo_data[&#39;Gender&#39;], demo_data[&#39;Ethnicity&#39;]]) . Gender Female Male . Ethnicity Asian Black Mixed-Black/Asian Mixed-White/Asian Mixed-White/Black Other White Asian Black Mixed-Black/Asian Mixed-White/Asian Mixed-White/Black Other White . Country . Australia 0 | 0 | 0 | 0 | 0 | 0 | 20 | 0 | 0 | 0 | 0 | 0 | 0 | 32 | . Canada 0 | 0 | 0 | 2 | 0 | 1 | 38 | 0 | 0 | 0 | 0 | 2 | 2 | 42 | . Ireland 0 | 0 | 0 | 0 | 0 | 0 | 9 | 0 | 0 | 0 | 0 | 0 | 0 | 11 | . New Zealand 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | . Other 1 | 1 | 0 | 0 | 2 | 2 | 30 | 1 | 0 | 1 | 0 | 1 | 5 | 74 | . UK 11 | 14 | 0 | 6 | 9 | 12 | 571 | 9 | 11 | 0 | 2 | 1 | 4 | 394 | . USA 3 | 2 | 2 | 3 | 0 | 13 | 184 | 0 | 5 | 0 | 7 | 4 | 23 | 305 | . Nearly no entry of Mixed-Black/Asian. (And compared to white, no entry of any other category) . | USA has no mixed-White/Black female, no asian and mixed-Black/Asian male. . | All the other countries have no mixed-black asian (except one male in an &#39;Other&#39; country) (even UK that has such a large number of respondents) . | UK has 9 times more Mixed-White/Black female than male and 3 times more &#39;Other&#39; female than male. . | . . Note: Again, plotting will not help because of the unbalance in white respondents. . Correlations (using Cram&#233;r&#39;s V) . &quot;In statistics, Cramér&#39;s V (sometimes referred to as Cramér&#39;s phi and denoted as φc) is a measure of association between two nominal variables, giving a value between 0 and +1 (inclusive). It is based on Pearson&#39;s chi-squared statistic and was published by Harald Cramér in 1946.&quot;(Wikipedia) . Below we define a set of useful functions for working with correlations. . def get_pval_code(pval): &#39;&#39;&#39; Returns a significance code string for a given p-value. &#39;&#39;&#39; code = &#39;&#39; if pval &lt; 0.001: code = &#39;***&#39; elif pval &lt; 0.01: code = &#39;**&#39; elif pval &lt; 0.05: code = &#39;*&#39; elif pval &lt; 0.1: code = &#39;.&#39; return code def display_pval_codes(): print(&quot;Significance codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1&quot;) . def cramers_v(crosstab): &#39;&#39;&#39; Returns Cramer&#39;s V correlation coefficient (and statistic significance) for data delivered as a crosstable. Used for nominal data. &#39;&#39;&#39; chi2 = chi2_contingency(crosstab)[0] n = crosstab.sum().sum() phi2 = chi2/n r,k = crosstab.shape phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1)) rcorr = r - ((r-1)**2)/(n-1) kcorr = k - ((k-1)**2)/(n-1) return round(np.sqrt(phi2corr / min((kcorr-1), (rcorr-1))), 3), chi2_contingency(crosstab)[1] . def nominal_corrs(dataset, col_1_names, col_2_names, pvalues=True): &#39;&#39;&#39; Returns Cramer&#39;s V coefficients matrix and p-value (obtained with Chi-square test of independence) matrix for the whole dataset. col_1_names, col_2_names - lists of names of columns to correlate. Function creates crosstables for every columns&#39; combination and returns a matrix with single Cramer&#39;s V coefficients of every combination; pvalues - if set to False, only correlation values will be returned in DataFrame (without &#39;**&#39; marks for significant observations) &#39;&#39;&#39; corr_table = pd.DataFrame() pval_table = pd.DataFrame() for i in range(len(col_1_names)): for j in range(len(col_2_names)): crosstab = pd.crosstab(dataset[col_1_names[i]], [dataset[col_2_names[j]]]) corr, pval = cramers_v(crosstab) v = &#39; &#39;.join([str(i) for i in (corr, get_pval_code(pval))]) if pvalues else corr corr_table.loc[i, j] = v pval_table.loc[i,j] = pval corr_table.index = col_1_names corr_table.columns = col_2_names pval_table.index = col_1_names pval_table.columns = col_2_names return corr_table, pval_table . def heatmap_corr(dataset, method=&#39;spearman&#39;, ready=False, mask=True, nominal=False): &#39;&#39;&#39; Extended sns.heatmap() method. dataset - can be &#39;pure&#39; data (without calculated correlations) or a DataFrame with already calcuateg correlations (in that case attribute &#39;ready&#39; should be set to True); method - mainly pearson or spearman; nominal correlations should be calculated externally and be delivered with attribute ready=True; mask - if dataset is NOT a cross-valued DataFrame of one type, mask should be set to False; nominal - for nominal data correlations values are in range (0, 1) instead of (-1, -1). nominal=True should be folowed by ready=True &#39;&#39;&#39; if not ready: corr = dataset.corr(method=method) elif ready: corr = dataset cmap = sns.diverging_palette(220, 10, as_cmap=True) vmax = corr.max().max() if nominal: center = 0.5 cmap=None elif not nominal: center = 0 if mask: mask = np.zeros_like(corr, dtype=np.bool) mask[np.triu_indices_from(mask)] = True np.fill_diagonal(corr.values, -2) vmax = corr.max().max() elif not mask: mask=None f, ax = plt.subplots(figsize=(20,9)) return sns.heatmap(corr, cmap=cmap, mask=mask, vmax=vmax, center=center, annot=True, square=True, linewidths=0.5, cbar_kws={&#39;shrink&#39;: 0.5}) . Let&#39;s display the correlations among the demographic data. . corr_table_demo, pval_table_demo = nominal_corrs(demo_data, demographic_columns, demographic_columns) display_pval_codes() corr_table_demo . Significance codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 . Age Gender Education Country Ethnicity . Age 1.0 *** | 0.196 *** | 0.229 *** | 0.173 *** | 0.014 | . Gender 0.196 *** | 0.999 *** | 0.235 *** | 0.219 *** | 0.0 | . Education 0.229 *** | 0.235 *** | 1.0 *** | 0.2 *** | 0.0 | . Country 0.173 *** | 0.219 *** | 0.2 *** | 1.0 *** | 0.053 ** | . Ethnicity 0.014 | 0.0 | 0.0 | 0.053 ** | 1.0 *** | . Values with at least one * next to them are statistically significant (p-value &lt; 0.05). . heatmap_corr(nominal_corrs(demo_data, demographic_columns, demographic_columns, pvalues=False)[0], nominal=True, ready=True); . Only very weak statistically significant correlations at most. . Important: A statistically significant correlation does not necessarily mean that the strength of the correlation is strong. A low p-value reassures us that the correlation is this way nearly all of the time. (It is the probability that we got it by chance) . This section tries to interpret these very weak relationships between variables . Ethnicity&#39;s lack of statistical significant correlations can be explained by the great imbalance in the various possible classes (all whites basically). Meaning the lack of correlation could be caused by chance. The abscence (high significance) of correlation with country may sound surprising but since the only countries considered are the &quot;same&quot;, as in developed and white prevalence in respondent, it makes sense. It&#39;s safe to assume the lack of correlation with age and gender are correct (even if unsignificant) for obvious reasons. I can&#39;t be so sure about education (as you can see below, there is around 50% chances the correlation was obtained by chance). . pval_table_demo.loc[&#39;Education&#39;, &#39;Ethnicity&#39;] . 0.5424195567242813 . There is a statistically significant very weak correlation between country and gender. We know, at least for the countries represented here (developped countries were population is basically 50/50), that there isn&#39;t a real life correlation between the country and gender. The very weak correlation may be due to some countries having more female repondents and some having more males (as we saw previously). . The weak correlation between age and gender can be explained, as we saw previously, that gender wasn&#39;t even for some young age groups but as we know this correlation doesn&#39;t generalize to real life. . A correlation between age and country would be relevant if some countries were more developped than the others but here we have only developped country. This idea would make more sense if we had more categories for elder people, meaning a country&#39;s life expectency could be taken into account, but here a single group for 65+ year olds isn&#39;t enough for developed countries. The very weak statistically significant relationship may be caused by the &#39;other&#39; countries. . The very weak correlations we can keep in mind are the following: . Age / Education (makes sense since basically no 18-24 would have a doctorate degree for example) | Gender / Education (males and females may have slightly different aims and standards regarding education) | Education / Country (for social tendencies, country development and other reasons, the education and country may be correlated but since the country here are very similar, the relationship is very weak) | . Personality features analysis . All the personnality traits have a score that is represented by a real value in the dataset. . Some have their original possible scores in ranges of integer types (with a step of 1, so 4-7 means 4, 5, 6 and 7): . Neuroticism: 12-60 | Extraversion: 16-59 | Openness to experience: 24-60 | Agreeableness: 12-60 | Conscientiousness: 17-59 | . pers_data = dataset[personality_columns] pers_data.describe() . Neuroticism Extraversion Openness to experience Agreeableness Conscientiousness Impulsiveness Sensation seeking . count 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | . mean -0.000551 | -0.001951 | -0.003224 | -0.000657 | -0.000394 | 0.005293 | -0.007408 | . std 0.998442 | 0.997418 | 0.995691 | 0.996689 | 0.997657 | 0.954148 | 0.962074 | . min -3.464360 | -3.273930 | -3.273930 | -3.464360 | -3.464360 | -2.555240 | -2.078480 | . 25% -0.678250 | -0.695090 | -0.717270 | -0.606330 | -0.652530 | -0.711260 | -0.525930 | . 50% 0.042570 | 0.003320 | -0.019280 | -0.017290 | -0.006650 | -0.217120 | 0.079870 | . 75% 0.629670 | 0.637790 | 0.723300 | 0.760960 | 0.584890 | 0.529750 | 0.765400 | . max 3.273930 | 3.273930 | 2.901610 | 3.464360 | 3.464360 | 2.901610 | 1.921730 | . Checking the balance / proportions . plot(&quot;pie&quot;, pers_data, personality_columns, fig_title=&quot;Pie plots personality traits&quot;) . The values overlap but we don&#39;t really care about them (I left them just in case you need them), we just want to see if some slices are bigger than the others. . plot(&quot;count&quot;, pers_data, personality_columns, show_percent=False, fig_title=&quot;Count / Frequency of personality data&quot;) . Hard to interpret since there are many levels for some personality traits and they are encoded as hardly interpretable real values, even though they are sorted in the same order as the original values, which aren&#39;t really interpretable either. . According to the pie plot, there doesn&#39;t seem to be any huge unbalance. . The count / frequency plot is more interesting. The distributions of all the personality traits may be gaussian distribution, with some skewness involved sometimes. Let&#39;s plot those densities a bit more nicely against actual estimated gaussian distributions to see if our intuition was right! . plot(&quot;density&quot;, pers_data, personality_columns, fig_title=&quot;Personality densities&quot;) . They are indeed belshaped but far from the shape of a fitted gaussian distribution. . Let&#39;s try out a statistical test. . def normal_test(dataset, columns = None, alpha = 0.05): if columns is None: columns = dataset.columns res = pd.DataFrame() data = dataset[columns] stats, pvals = data.apply(shapiro).values return pd.DataFrame(data=[(&quot;Not rejected&quot; if pval &gt; alpha else &quot;Rejected&quot;) + f&quot; with alpha = {alpha}&quot; for pval in pvals], columns=[&quot;H0: is from normal distribution&quot;], index=columns) . normal_test(pers_data) . H0: is from normal distribution . Neuroticism Not rejected with alpha = 0.05 | . Extraversion Rejected with alpha = 0.05 | . Openness to experience Rejected with alpha = 0.05 | . Agreeableness Rejected with alpha = 0.05 | . Conscientiousness Rejected with alpha = 0.05 | . Impulsiveness Rejected with alpha = 0.05 | . Sensation seeking Rejected with alpha = 0.05 | . After running the Shapiro-Wilk test on each personality trait, the only hypothesis that couldn&#39;t be rejected was &quot;Neuroticism is from a normal distribution&quot; with an alpha = 0.05. . Warning: That doesn&#8217;t mean it is gaussian. Actually the probability was something around 10%, but we can&#8217;t reject this hypothesis. . Correlations among personality traits (using Spearman) . &quot;In statistics, Spearman&#39;s rank correlation coefficient or Spearman&#39;s ρ, named after Charles Spearman, is a nonparametric measure of rank correlation (statistical dependence between the rankings of two variables). ... Spearman&#39;s correlation assesses monotonic relationships (whether linear or not). If there are no repeated data values, a perfect Spearman correlation of +1 or −1 occurs when each of the variables is a perfect monotone function of the other.&quot;(Wikipedia) . Again, we have to define a few useful functions. . def calculate_r(df1, df2, method=&#39;spearman&#39;, pvalues=True): &#39;&#39;&#39; Returns correlation matrix extended by statistical significance index. Used for non-nominal data. df1, df2 - DataFrames of data to correlate; method - mainly pearson and spearman; pvalues - if set to False, only correlation values will be returned in DataFrame (without &#39;**&#39; marks for significant observations) &#39;&#39;&#39; float_precision = 3 data_corr_table = pd.DataFrame() data_pvalues = pd.DataFrame() for x in df1.columns: for y in df2.columns: if method == &#39;pearson&#39;: corr = pearsonr(df1[x], df2[y]) elif method == &#39;spearman&#39;: corr = spearmanr(df1[x], df2[y]) else: raise ValueError(&#39;Unknown method&#39;) if pvalues: data_corr_table.loc[x,y] = &#39;{} {}&#39;.format(round(corr[0], float_precision), get_pval_code(corr[1])) elif not pvalues: data_corr_table.loc[x,y] = round(corr[0], float_precision) data_pvalues.loc[x,y] = corr[1] return data_corr_table, data_pvalues . def get_ranked_corr(correlations, pvalues, pval_threshold=0.05, corr_threshold=0.5, name_transform=lambda x: x, against_itself=True, comp_corr = operator.ge, comp_pval = operator.le): &#39;&#39;&#39; Returns the sorted top (wanted) correlations as a Dataframe correlations - the correlations dataframe. pvalues - The pvalues of H0: the correlation is unsignificant. pval_threshold - alpha to compare to pvalue corr_threshold - Threadshold of correlations to consider (upper or lower depending on comp_corr) name_transform - transform the column names before writing index. against_itself - True if columns and rows are the same data (if symmetric matrix) comp_corr - comparator to compare to the corr_threshold (default operator.ge means we want correlations &gt;=) comp_pval - comparator to compare the pvalue (default operator.le meaning we want pvalues &lt;=) &#39;&#39;&#39; columns = correlations.columns rows = correlations.index pairs=[] for i, row in enumerate(rows): for j, col in enumerate(columns[i+1:], start=i+1) if against_itself else enumerate(columns): corr = correlations.iloc[i, j] if comp_pval(pvalues.iloc[i,j], pval_threshold) and comp_corr(abs(corr), corr_threshold): pairs.append((name_transform(row), name_transform(col), corr)) list.sort(pairs, key=lambda x: abs(x[2]), reverse=True) return pd.Series(data=[x[2] for x in pairs], index=[&#39; / &#39;.join((x[0], x[1])) for x in pairs], name=&quot;Correlation&quot;, dtype=np.float64).to_frame() . Now let&#39;s use them. . pers_data_corr_table, pers_data_pvalues = calculate_r(pers_data, pers_data, method=&#39;spearman&#39;) pers_data_corr_table . Neuroticism Extraversion Openness to experience Agreeableness Conscientiousness Impulsiveness Sensation seeking . Neuroticism 1.0 *** | -0.417 *** | 0.015 | -0.206 *** | -0.378 *** | 0.168 *** | 0.09 *** | . Extraversion -0.417 *** | 1.0 *** | 0.226 *** | 0.162 *** | 0.29 *** | 0.119 *** | 0.19 *** | . Openness to experience 0.015 | 0.226 *** | 1.0 *** | 0.037 | -0.073 ** | 0.27 *** | 0.405 *** | . Agreeableness -0.206 *** | 0.162 *** | 0.037 | 1.0 *** | 0.237 *** | -0.226 *** | -0.208 *** | . Conscientiousness -0.378 *** | 0.29 *** | -0.073 ** | 0.237 *** | 1.0 *** | -0.344 *** | -0.254 *** | . Impulsiveness 0.168 *** | 0.119 *** | 0.27 *** | -0.226 *** | -0.344 *** | 1.0 *** | 0.628 *** | . Sensation seeking 0.09 *** | 0.19 *** | 0.405 *** | -0.208 *** | -0.254 *** | 0.628 *** | 1.0 *** | . heatmap_corr(pers_data, method=&#39;spearman&#39;); . Again, no strong correlations but some moderate and weak-moderate ones. The sign of the correlation is interesting here. . Neuroticism for example has no positive correlation with any other personality trait. It has a weak-moderate downhill correlation with extraversion and conscientiousness. . Tip: If you ever feel moody, anxious, frustrated, lonely, depressed or other such things, try to be more outgoing, talkative, energetic, careful and diligent and see how it goes! :wink: . I also found it interesting that Extraversion has no moderate correlation with any other personality trait (and that its strongest correlation, which is weak, is with conscentiousness). . Below is a list of the very weak and above statistically significant correlations. (|r| &gt;= 0.15) . pers_data_corr_table_nopval, _ = calculate_r(pers_data, pers_data, method=&#39;spearman&#39;, pvalues=False) . get_ranked_corr(pers_data_corr_table_nopval, pers_data_pvalues, corr_threshold=0.15) . Correlation . Impulsiveness / Sensation seeking 0.628 | . Neuroticism / Extraversion -0.417 | . Openness to experience / Sensation seeking 0.405 | . Neuroticism / Conscientiousness -0.378 | . Conscientiousness / Impulsiveness -0.344 | . Extraversion / Conscientiousness 0.290 | . Openness to experience / Impulsiveness 0.270 | . Conscientiousness / Sensation seeking -0.254 | . Agreeableness / Conscientiousness 0.237 | . Extraversion / Openness to experience 0.226 | . Agreeableness / Impulsiveness -0.226 | . Agreeableness / Sensation seeking -0.208 | . Neuroticism / Agreeableness -0.206 | . Extraversion / Sensation seeking 0.190 | . Neuroticism / Impulsiveness 0.168 | . Extraversion / Agreeableness 0.162 | . Drug consumption analysis . As a reminder, here are the different levels for each drug: . 0 - Never Used . 1 - Used over a Decade Ago . 2 - Used in the Last Decade . 3 - Used in the Last Year . 4 - Used in the Last Month . 5 - Used in the Last Week . 6 - Used in the Last Day . drug_data = dataset[drugs_columns] . drug_data.describe() . Alcohol consumption Amphetamines consumption Amyl nitrite consumption Benzodiazepine consumption Caffeine consumption Cannabis consumption Chocolate consumption Cocaine consumption Crack consumption Ecstasy consumption Heroin consumption Ketamine consumption Legal highs consumption Lysergic acid diethylamide consumption Methadone consumption Magic mushrooms consumption Nicotine consumption Volatile substance abuse consumption . count 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | 1877.000000 | . mean 4.637720 | 1.337773 | 0.606819 | 1.461907 | 5.484283 | 2.980288 | 5.109750 | 1.156633 | 0.295685 | 1.309536 | 0.372403 | 0.564198 | 1.353223 | 1.053277 | 0.826319 | 1.177944 | 3.194992 | 0.429409 | . std 1.328234 | 1.782384 | 1.064005 | 1.869193 | 1.115119 | 2.286778 | 1.085716 | 1.510791 | 0.835925 | 1.647373 | 1.034761 | 1.216341 | 1.790881 | 1.484582 | 1.648379 | 1.459212 | 2.415814 | 0.959160 | . min 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 4.000000 | 0.000000 | 0.000000 | 0.000000 | 5.000000 | 1.000000 | 5.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 0.000000 | . 50% 5.000000 | 0.000000 | 0.000000 | 0.000000 | 6.000000 | 3.000000 | 5.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 3.000000 | 0.000000 | . 75% 6.000000 | 2.000000 | 1.000000 | 3.000000 | 6.000000 | 5.000000 | 6.000000 | 2.000000 | 0.000000 | 3.000000 | 0.000000 | 0.000000 | 3.000000 | 2.000000 | 0.000000 | 2.000000 | 6.000000 | 0.000000 | . max 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | . Let&#39;s make a table with the number of observation and frequency for every consumption level of every drug. . counts = [] proportions = [] for drug in drugs_columns: counts.append(drug_data[drug].value_counts()) proportions.append(drug_data[drug].value_counts(normalize=True)) drug_table_count = pd.concat(counts, axis=1, keys=drugs_columns, sort=True) drug_table_prop = (pd.concat(proportions, axis=1, keys=drugs_columns, sort=True) * 100).round(2) drug_table = pd.concat([drug_table_count.T, drug_table_prop.T], keys=[&#39;N&#39;, &#39;%&#39;], axis=1) drug_table . N % . 0 1 2 3 4 5 6 0 1 2 3 4 5 6 . Alcohol consumption 33 | 34 | 68 | 197 | 284 | 758 | 503 | 1.76 | 1.81 | 3.62 | 10.50 | 15.13 | 40.38 | 26.80 | . Amphetamines consumption 973 | 230 | 241 | 196 | 75 | 61 | 101 | 51.84 | 12.25 | 12.84 | 10.44 | 4.00 | 3.25 | 5.38 | . Amyl nitrite consumption 1299 | 210 | 236 | 91 | 24 | 14 | 3 | 69.21 | 11.19 | 12.57 | 4.85 | 1.28 | 0.75 | 0.16 | . Benzodiazepine consumption 999 | 116 | 230 | 234 | 119 | 84 | 95 | 53.22 | 6.18 | 12.25 | 12.47 | 6.34 | 4.48 | 5.06 | . Caffeine consumption 27 | 10 | 24 | 59 | 106 | 271 | 1380 | 1.44 | 0.53 | 1.28 | 3.14 | 5.65 | 14.44 | 73.52 | . Cannabis consumption 413 | 207 | 266 | 210 | 138 | 185 | 458 | 22.00 | 11.03 | 14.17 | 11.19 | 7.35 | 9.86 | 24.40 | . Chocolate consumption 32 | 2 | 10 | 53 | 295 | 680 | 805 | 1.70 | 0.11 | 0.53 | 2.82 | 15.72 | 36.23 | 42.89 | . Cocaine consumption 1036 | 160 | 267 | 257 | 98 | 40 | 19 | 55.19 | 8.52 | 14.22 | 13.69 | 5.22 | 2.13 | 1.01 | . Crack consumption 1622 | 67 | 109 | 59 | 9 | 9 | 2 | 86.41 | 3.57 | 5.81 | 3.14 | 0.48 | 0.48 | 0.11 | . Ecstasy consumption 1020 | 112 | 232 | 275 | 154 | 63 | 21 | 54.34 | 5.97 | 12.36 | 14.65 | 8.20 | 3.36 | 1.12 | . Heroin consumption 1600 | 68 | 91 | 65 | 24 | 16 | 13 | 85.24 | 3.62 | 4.85 | 3.46 | 1.28 | 0.85 | 0.69 | . Ketamine consumption 1488 | 43 | 140 | 129 | 40 | 33 | 4 | 79.28 | 2.29 | 7.46 | 6.87 | 2.13 | 1.76 | 0.21 | . Legal highs consumption 1092 | 29 | 195 | 321 | 109 | 64 | 67 | 58.18 | 1.55 | 10.39 | 17.10 | 5.81 | 3.41 | 3.57 | . Lysergic acid diethylamide consumption 1069 | 257 | 175 | 213 | 96 | 55 | 12 | 56.95 | 13.69 | 9.32 | 11.35 | 5.11 | 2.93 | 0.64 | . Methadone consumption 1424 | 39 | 95 | 148 | 50 | 48 | 73 | 75.87 | 2.08 | 5.06 | 7.88 | 2.66 | 2.56 | 3.89 | . Magic mushrooms consumption 982 | 208 | 259 | 272 | 114 | 39 | 3 | 52.32 | 11.08 | 13.80 | 14.49 | 6.07 | 2.08 | 0.16 | . Nicotine consumption 428 | 193 | 203 | 184 | 106 | 156 | 607 | 22.80 | 10.28 | 10.82 | 9.80 | 5.65 | 8.31 | 32.34 | . Volatile substance abuse consumption 1452 | 199 | 133 | 59 | 13 | 14 | 7 | 77.36 | 10.60 | 7.09 | 3.14 | 0.69 | 0.75 | 0.37 | . We will also plot them for each drug. . plot(&quot;count&quot;, drug_data, fig_title=&quot;Count / Frequency for drug consumption n n&quot;, rotate_label=False) . Alcohol, caffeine and chocolate are legal stimulants that are consumed reguarly by the vast majority of respondents. . Cannabis and nicotine are distributed pretty evenly, even if there is a higher proportion at both extremes (0 and 6 category). These two are are considered bad or ok depending on the people, this may explain the about equal number of daily consumers and those who never tried it, with some in between. . Other illegal drugs were never tried for the vast majority of respondents, or tried a long time ago (considered as more than a month ago) People probably either never tried it (fear or convictions) or did it once but never again. This also holds for legal highs, which follows the same trend as the illegal drugs. . Correlations among drugs (using Spearman) . drug_data_corr_table, drug_data_pvalues = calculate_r(drug_data, drug_data, method=&#39;spearman&#39;) display_pval_codes() drug_data_corr_table . Significance codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 . Alcohol consumption Amphetamines consumption Amyl nitrite consumption Benzodiazepine consumption Caffeine consumption Cannabis consumption Chocolate consumption Cocaine consumption Crack consumption Ecstasy consumption Heroin consumption Ketamine consumption Legal highs consumption Lysergic acid diethylamide consumption Methadone consumption Magic mushrooms consumption Nicotine consumption Volatile substance abuse consumption . Alcohol consumption 1.0 *** | -0.019 | 0.076 ** | -0.03 | 0.103 *** | 0.003 | 0.01 | 0.054 * | -0.056 * | 0.039 . | -0.054 * | 0.048 * | 0.01 | -0.017 | -0.08 *** | -0.0 | 0.043 . | -0.002 | . Amphetamines consumption -0.019 | 1.0 *** | 0.365 *** | 0.505 *** | 0.022 | 0.504 *** | -0.093 *** | 0.606 *** | 0.338 *** | 0.579 *** | 0.382 *** | 0.402 *** | 0.481 *** | 0.499 *** | 0.422 *** | 0.484 *** | 0.395 *** | 0.327 *** | . Amyl nitrite consumption 0.076 ** | 0.365 *** | 1.0 *** | 0.219 *** | 0.061 ** | 0.217 *** | 0.005 | 0.395 *** | 0.178 *** | 0.359 *** | 0.169 *** | 0.344 *** | 0.216 *** | 0.208 *** | 0.078 *** | 0.236 *** | 0.228 *** | 0.188 *** | . Benzodiazepine consumption -0.03 | 0.505 *** | 0.219 *** | 1.0 *** | 0.052 * | 0.37 *** | -0.053 * | 0.467 *** | 0.372 *** | 0.373 *** | 0.437 *** | 0.314 *** | 0.345 *** | 0.37 *** | 0.515 *** | 0.374 *** | 0.318 *** | 0.314 *** | . Caffeine consumption 0.103 *** | 0.022 | 0.061 ** | 0.052 * | 1.0 *** | -0.01 | 0.109 *** | 0.036 | 0.011 | -0.037 | 0.006 | -0.033 | -0.052 * | -0.038 . | 0.014 | -0.004 | 0.101 *** | 0.03 | . Cannabis consumption 0.003 | 0.504 *** | 0.217 *** | 0.37 *** | -0.01 | 1.0 *** | -0.108 *** | 0.477 *** | 0.24 *** | 0.566 *** | 0.239 *** | 0.319 *** | 0.578 *** | 0.542 *** | 0.322 *** | 0.593 *** | 0.515 *** | 0.29 *** | . Chocolate consumption 0.01 | -0.093 *** | 0.005 | -0.053 * | 0.109 *** | -0.108 *** | 1.0 *** | -0.088 *** | -0.126 *** | -0.091 *** | -0.083 *** | -0.053 * | -0.098 *** | -0.122 *** | -0.061 ** | -0.108 *** | -0.058 * | -0.092 *** | . Cocaine consumption 0.054 * | 0.606 *** | 0.395 *** | 0.467 *** | 0.036 | 0.477 *** | -0.088 *** | 1.0 *** | 0.406 *** | 0.631 *** | 0.428 *** | 0.465 *** | 0.42 *** | 0.464 *** | 0.378 *** | 0.469 *** | 0.414 *** | 0.322 *** | . Crack consumption -0.056 * | 0.338 *** | 0.178 *** | 0.372 *** | 0.011 | 0.24 *** | -0.126 *** | 0.406 *** | 1.0 *** | 0.26 *** | 0.539 *** | 0.269 *** | 0.199 *** | 0.308 *** | 0.391 *** | 0.293 *** | 0.256 *** | 0.305 *** | . Ecstasy consumption 0.039 . | 0.579 *** | 0.359 *** | 0.373 *** | -0.037 | 0.566 *** | -0.091 *** | 0.631 *** | 0.26 *** | 1.0 *** | 0.29 *** | 0.523 *** | 0.57 *** | 0.581 *** | 0.313 *** | 0.565 *** | 0.392 *** | 0.282 *** | . Heroin consumption -0.054 * | 0.382 *** | 0.169 *** | 0.437 *** | 0.006 | 0.239 *** | -0.083 *** | 0.428 *** | 0.539 *** | 0.29 *** | 1.0 *** | 0.297 *** | 0.244 *** | 0.345 *** | 0.501 *** | 0.298 *** | 0.246 *** | 0.311 *** | . Ketamine consumption 0.048 * | 0.402 *** | 0.344 *** | 0.314 *** | -0.033 | 0.319 *** | -0.053 * | 0.465 *** | 0.269 *** | 0.523 *** | 0.297 *** | 1.0 *** | 0.405 *** | 0.413 *** | 0.236 *** | 0.409 *** | 0.256 *** | 0.207 *** | . Legal highs consumption 0.01 | 0.481 *** | 0.216 *** | 0.345 *** | -0.052 * | 0.578 *** | -0.098 *** | 0.42 *** | 0.199 *** | 0.57 *** | 0.244 *** | 0.405 *** | 1.0 *** | 0.478 *** | 0.35 *** | 0.551 *** | 0.348 *** | 0.308 *** | . Lysergic acid diethylamide consumption -0.017 | 0.499 *** | 0.208 *** | 0.37 *** | -0.038 . | 0.542 *** | -0.122 *** | 0.464 *** | 0.308 *** | 0.581 *** | 0.345 *** | 0.413 *** | 0.478 *** | 1.0 *** | 0.34 *** | 0.672 *** | 0.309 *** | 0.291 *** | . Methadone consumption -0.08 *** | 0.422 *** | 0.078 *** | 0.515 *** | 0.014 | 0.322 *** | -0.061 ** | 0.378 *** | 0.391 *** | 0.313 *** | 0.501 *** | 0.236 *** | 0.35 *** | 0.34 *** | 1.0 *** | 0.333 *** | 0.255 *** | 0.308 *** | . Magic mushrooms consumption -0.0 | 0.484 *** | 0.236 *** | 0.374 *** | -0.004 | 0.593 *** | -0.108 *** | 0.469 *** | 0.293 *** | 0.565 *** | 0.298 *** | 0.409 *** | 0.551 *** | 0.672 *** | 0.333 *** | 1.0 *** | 0.338 *** | 0.263 *** | . Nicotine consumption 0.043 . | 0.395 *** | 0.228 *** | 0.318 *** | 0.101 *** | 0.515 *** | -0.058 * | 0.414 *** | 0.256 *** | 0.392 *** | 0.246 *** | 0.256 *** | 0.348 *** | 0.309 *** | 0.255 *** | 0.338 *** | 1.0 *** | 0.294 *** | . Volatile substance abuse consumption -0.002 | 0.327 *** | 0.188 *** | 0.314 *** | 0.03 | 0.29 *** | -0.092 *** | 0.322 *** | 0.305 *** | 0.282 *** | 0.311 *** | 0.207 *** | 0.308 *** | 0.291 *** | 0.308 *** | 0.263 *** | 0.294 *** | 1.0 *** | . heatmap_corr(drug_data, method=&#39;spearman&#39;); . Alcohol, caffeine and chocolate don&#39;t correlate with any other drugs (too weak or insignificant relations). . There are some moderate correlations with high enough statistical significance. . There are no strong correlations (I consider an absolute value of 0.70 or above strong). . drug_data_corr_table_nopval, _ = calculate_r(drug_data, drug_data, method=&#39;spearman&#39;, pvalues=False) . We will list every correlations, regrouped and ordered by strength. . Tip: Feel free to scroll past it, I just included them because I thought it might be of some interest for some people. . List of unsignificant correlations . get_ranked_corr(drug_data_corr_table_nopval, drug_data_pvalues, corr_threshold=0, name_transform=lambda x:x.rsplit(&#39; &#39;, 1)[0], comp_pval= operator.gt).drop(&#39;Correlation&#39;, axis=1) . . Alcohol / Nicotine . Alcohol / Ecstasy . Caffeine / Lysergic acid diethylamide . Caffeine / Ecstasy . Caffeine / Cocaine . Caffeine / Ketamine . Alcohol / Benzodiazepine . Caffeine / Volatile substance abuse . Amphetamines / Caffeine . Alcohol / Amphetamines . Alcohol / Lysergic acid diethylamide . Caffeine / Methadone . Caffeine / Crack . Alcohol / Chocolate . Alcohol / Legal highs . Caffeine / Cannabis . Caffeine / Heroin . Amyl nitrite / Chocolate . Caffeine / Magic mushrooms . Alcohol / Cannabis . Alcohol / Volatile substance abuse . Alcohol / Magic mushrooms . List of significant weak or non-existant correlations . get_ranked_corr(drug_data_corr_table_nopval, drug_data_pvalues, corr_threshold=0.3, name_transform=lambda x:x.rsplit(&#39; &#39;, 1)[0], comp_corr=operator.lt) . Correlation . Heroin / Magic mushrooms 0.298 | . Heroin / Ketamine 0.297 | . Nicotine / Volatile substance abuse 0.294 | . Crack / Magic mushrooms 0.293 | . Lysergic acid diethylamide / Volatile substance abuse 0.291 | . Cannabis / Volatile substance abuse 0.290 | . Ecstasy / Heroin 0.290 | . Ecstasy / Volatile substance abuse 0.282 | . Crack / Ketamine 0.269 | . Magic mushrooms / Volatile substance abuse 0.263 | . Crack / Ecstasy 0.260 | . Crack / Nicotine 0.256 | . Ketamine / Nicotine 0.256 | . Methadone / Nicotine 0.255 | . Heroin / Nicotine 0.246 | . Heroin / Legal highs 0.244 | . Cannabis / Crack 0.240 | . Cannabis / Heroin 0.239 | . Amyl nitrite / Magic mushrooms 0.236 | . Ketamine / Methadone 0.236 | . Amyl nitrite / Nicotine 0.228 | . Amyl nitrite / Benzodiazepine 0.219 | . Amyl nitrite / Cannabis 0.217 | . Amyl nitrite / Legal highs 0.216 | . Amyl nitrite / Lysergic acid diethylamide 0.208 | . Ketamine / Volatile substance abuse 0.207 | . Crack / Legal highs 0.199 | . Amyl nitrite / Volatile substance abuse 0.188 | . Amyl nitrite / Crack 0.178 | . Amyl nitrite / Heroin 0.169 | . Chocolate / Crack -0.126 | . Chocolate / Lysergic acid diethylamide -0.122 | . Caffeine / Chocolate 0.109 | . Cannabis / Chocolate -0.108 | . Chocolate / Magic mushrooms -0.108 | . Alcohol / Caffeine 0.103 | . Caffeine / Nicotine 0.101 | . Chocolate / Legal highs -0.098 | . Amphetamines / Chocolate -0.093 | . Chocolate / Volatile substance abuse -0.092 | . Chocolate / Ecstasy -0.091 | . Chocolate / Cocaine -0.088 | . Chocolate / Heroin -0.083 | . Alcohol / Methadone -0.080 | . Amyl nitrite / Methadone 0.078 | . Alcohol / Amyl nitrite 0.076 | . Amyl nitrite / Caffeine 0.061 | . Chocolate / Methadone -0.061 | . Chocolate / Nicotine -0.058 | . Alcohol / Crack -0.056 | . Alcohol / Cocaine 0.054 | . Alcohol / Heroin -0.054 | . Benzodiazepine / Chocolate -0.053 | . Chocolate / Ketamine -0.053 | . Benzodiazepine / Caffeine 0.052 | . Caffeine / Legal highs -0.052 | . Alcohol / Ketamine 0.048 | . List of significant weak to weak-moderate correlations . get_ranked_corr(drug_data_corr_table_nopval, drug_data_pvalues, corr_threshold=0.5, name_transform=lambda x:x.rsplit(&#39; &#39;, 1)[0], comp_corr=operator.lt).query(&#39;Correlation &gt;= 0.3&#39;).reset_index() . index Correlation . 0 Amphetamines / Lysergic acid diethylamide | 0.499 | . 1 Amphetamines / Magic mushrooms | 0.484 | . 2 Amphetamines / Legal highs | 0.481 | . 3 Legal highs / Lysergic acid diethylamide | 0.478 | . 4 Cannabis / Cocaine | 0.477 | . 5 Cocaine / Magic mushrooms | 0.469 | . 6 Benzodiazepine / Cocaine | 0.467 | . 7 Cocaine / Ketamine | 0.465 | . 8 Cocaine / Lysergic acid diethylamide | 0.464 | . 9 Benzodiazepine / Heroin | 0.437 | . 10 Cocaine / Heroin | 0.428 | . 11 Amphetamines / Methadone | 0.422 | . 12 Cocaine / Legal highs | 0.420 | . 13 Cocaine / Nicotine | 0.414 | . 14 Ketamine / Lysergic acid diethylamide | 0.413 | . 15 Ketamine / Magic mushrooms | 0.409 | . 16 Cocaine / Crack | 0.406 | . 17 Ketamine / Legal highs | 0.405 | . 18 Amphetamines / Ketamine | 0.402 | . 19 Amphetamines / Nicotine | 0.395 | . 20 Amyl nitrite / Cocaine | 0.395 | . 21 Ecstasy / Nicotine | 0.392 | . 22 Crack / Methadone | 0.391 | . 23 Amphetamines / Heroin | 0.382 | . 24 Cocaine / Methadone | 0.378 | . 25 Benzodiazepine / Magic mushrooms | 0.374 | . 26 Benzodiazepine / Ecstasy | 0.373 | . 27 Benzodiazepine / Crack | 0.372 | . 28 Benzodiazepine / Cannabis | 0.370 | . 29 Benzodiazepine / Lysergic acid diethylamide | 0.370 | . 30 Amphetamines / Amyl nitrite | 0.365 | . 31 Amyl nitrite / Ecstasy | 0.359 | . 32 Legal highs / Methadone | 0.350 | . 33 Legal highs / Nicotine | 0.348 | . 34 Benzodiazepine / Legal highs | 0.345 | . 35 Heroin / Lysergic acid diethylamide | 0.345 | . 36 Amyl nitrite / Ketamine | 0.344 | . 37 Lysergic acid diethylamide / Methadone | 0.340 | . 38 Amphetamines / Crack | 0.338 | . 39 Magic mushrooms / Nicotine | 0.338 | . 40 Methadone / Magic mushrooms | 0.333 | . 41 Amphetamines / Volatile substance abuse | 0.327 | . 42 Cannabis / Methadone | 0.322 | . 43 Cocaine / Volatile substance abuse | 0.322 | . 44 Cannabis / Ketamine | 0.319 | . 45 Benzodiazepine / Nicotine | 0.318 | . 46 Benzodiazepine / Ketamine | 0.314 | . 47 Benzodiazepine / Volatile substance abuse | 0.314 | . 48 Ecstasy / Methadone | 0.313 | . 49 Heroin / Volatile substance abuse | 0.311 | . 50 Lysergic acid diethylamide / Nicotine | 0.309 | . 51 Crack / Lysergic acid diethylamide | 0.308 | . 52 Legal highs / Volatile substance abuse | 0.308 | . 53 Methadone / Volatile substance abuse | 0.308 | . 54 Crack / Volatile substance abuse | 0.305 | . List of significant moderate correlations . get_ranked_corr(drug_data_corr_table_nopval, drug_data_pvalues, corr_threshold=0.4, name_transform=lambda x:x.rsplit(&#39; &#39;, 1)[0]) . Correlation . Lysergic acid diethylamide / Magic mushrooms 0.672 | . Cocaine / Ecstasy 0.631 | . Amphetamines / Cocaine 0.606 | . Cannabis / Magic mushrooms 0.593 | . Ecstasy / Lysergic acid diethylamide 0.581 | . Amphetamines / Ecstasy 0.579 | . Cannabis / Legal highs 0.578 | . Ecstasy / Legal highs 0.570 | . Cannabis / Ecstasy 0.566 | . Ecstasy / Magic mushrooms 0.565 | . Legal highs / Magic mushrooms 0.551 | . Cannabis / Lysergic acid diethylamide 0.542 | . Crack / Heroin 0.539 | . Ecstasy / Ketamine 0.523 | . Benzodiazepine / Methadone 0.515 | . Cannabis / Nicotine 0.515 | . Amphetamines / Benzodiazepine 0.505 | . Amphetamines / Cannabis 0.504 | . Heroin / Methadone 0.501 | . Amphetamines / Lysergic acid diethylamide 0.499 | . Amphetamines / Magic mushrooms 0.484 | . Amphetamines / Legal highs 0.481 | . Legal highs / Lysergic acid diethylamide 0.478 | . Cannabis / Cocaine 0.477 | . Cocaine / Magic mushrooms 0.469 | . Benzodiazepine / Cocaine 0.467 | . Cocaine / Ketamine 0.465 | . Cocaine / Lysergic acid diethylamide 0.464 | . Benzodiazepine / Heroin 0.437 | . Cocaine / Heroin 0.428 | . Amphetamines / Methadone 0.422 | . Cocaine / Legal highs 0.420 | . Cocaine / Nicotine 0.414 | . Ketamine / Lysergic acid diethylamide 0.413 | . Ketamine / Magic mushrooms 0.409 | . Cocaine / Crack 0.406 | . Ketamine / Legal highs 0.405 | . Amphetamines / Ketamine 0.402 | . We have seen correlations of categories of variables with themselves. Let&#39;s now dig into inter-category correlations. . Correlations between personality traits and drugs (using Spearman) . drug_pers_corr_table, drug_pers_pvalues = calculate_r(drug_data, pers_data, method=&#39;spearman&#39;) drug_pers_corr_table . Neuroticism Extraversion Openness to experience Agreeableness Conscientiousness Impulsiveness Sensation seeking . Alcohol consumption 0.0 | 0.084 *** | 0.032 | -0.033 | 0.006 | 0.039 . | 0.094 *** | . Amphetamines consumption 0.135 *** | -0.042 . | 0.253 *** | -0.14 *** | -0.253 *** | 0.294 *** | 0.365 *** | . Amyl nitrite consumption 0.04 . | 0.04 . | 0.065 ** | -0.079 *** | -0.121 *** | 0.13 *** | 0.189 *** | . Benzodiazepine consumption 0.266 *** | -0.096 *** | 0.225 *** | -0.164 *** | -0.217 *** | 0.233 *** | 0.255 *** | . Caffeine consumption 0.022 | 0.013 | -0.02 | -0.008 | 0.006 | 0.014 | -0.005 | . Cannabis consumption 0.11 *** | -0.022 | 0.417 *** | -0.155 *** | -0.293 *** | 0.313 *** | 0.465 *** | . Chocolate consumption 0.025 | 0.028 | -0.015 | 0.038 | 0.023 | -0.015 | -0.052 * | . Cocaine consumption 0.144 *** | 0.006 | 0.205 *** | -0.183 *** | -0.221 *** | 0.264 *** | 0.34 *** | . Crack consumption 0.118 *** | -0.052 * | 0.126 *** | -0.091 *** | -0.13 *** | 0.19 *** | 0.197 *** | . Ecstasy consumption 0.087 *** | 0.056 * | 0.311 *** | -0.118 *** | -0.249 *** | 0.272 *** | 0.405 *** | . Heroin consumption 0.178 *** | -0.076 *** | 0.163 *** | -0.14 *** | -0.17 *** | 0.198 *** | 0.216 *** | . Ketamine consumption 0.078 *** | 0.019 | 0.193 *** | -0.125 *** | -0.167 *** | 0.188 *** | 0.266 *** | . Legal highs consumption 0.122 *** | -0.033 | 0.347 *** | -0.136 *** | -0.27 *** | 0.281 *** | 0.441 *** | . Lysergic acid diethylamide consumption 0.071 ** | -0.021 | 0.368 *** | -0.12 *** | -0.198 *** | 0.253 *** | 0.381 *** | . Methadone consumption 0.194 *** | -0.108 *** | 0.209 *** | -0.153 *** | -0.209 *** | 0.198 *** | 0.242 *** | . Magic mushrooms consumption 0.066 ** | 0.001 | 0.379 *** | -0.127 *** | -0.221 *** | 0.281 *** | 0.391 *** | . Nicotine consumption 0.133 *** | -0.033 | 0.193 *** | -0.113 *** | -0.237 *** | 0.253 *** | 0.306 *** | . Volatile substance abuse consumption 0.132 *** | -0.066 ** | 0.151 *** | -0.125 *** | -0.182 *** | 0.192 *** | 0.255 *** | . drug_pers_corr_table_no_pvalues, drug_pers_pvalues = calculate_r(drug_data, pers_data, pvalues=False, method=&#39;spearman&#39;) heatmap_corr(drug_pers_corr_table_no_pvalues, ready=True, mask=False); . Alcohol, Chocolate, Caffeine and Nicotine consumption have no (or unsignificant) correlation with any personality trait. . For the rest of this paragraph, &quot;drug consumption&quot; is not taking alcohol, chocolate or caffeine into account. . Extraversion is the only personality trait that has no correlation with any drug (not even very weak), which I found surpising at first but it may be caused about my wrong understanding of extraversion. . Crack, and Amyl nitrite have no correlation above 0.2 (absolute value). . Agreeableness and Conscientiousness both have only weak negative correlations (at most) with all the drugs. Even if the negative correlation is always a bit stronger for conscientious respondents. . Impulsiveness, Openness to experience and Sensation seeking are the personality factors that correlate the most with drug consumption. . There are no strong correlation between personality factors and drug taking. . It seems like a particular personality trait has either only positive or negative correlations with drug consumption. . Let&#39;s list the weak ones and above (|r| &gt;= 0.3). . get_ranked_corr(drug_pers_corr_table_no_pvalues, drug_pers_pvalues, corr_threshold=0.3, against_itself=False) . Correlation . Cannabis consumption / Sensation seeking 0.465 | . Legal highs consumption / Sensation seeking 0.441 | . Cannabis consumption / Openness to experience 0.417 | . Ecstasy consumption / Sensation seeking 0.405 | . Magic mushrooms consumption / Sensation seeking 0.391 | . Lysergic acid diethylamide consumption / Sensation seeking 0.381 | . Magic mushrooms consumption / Openness to experience 0.379 | . Lysergic acid diethylamide consumption / Openness to experience 0.368 | . Amphetamines consumption / Sensation seeking 0.365 | . Legal highs consumption / Openness to experience 0.347 | . Cocaine consumption / Sensation seeking 0.340 | . Cannabis consumption / Impulsiveness 0.313 | . Ecstasy consumption / Openness to experience 0.311 | . Nicotine consumption / Sensation seeking 0.306 | . . Note: There are only positive correlations in the above list. The strongest negative is -0.293 between Cannabis and conscientiousness. . Nominal correlations between demographic and drug consumption (Cram&#233;r&#39;s V) . demo_drug_corr_table, demo_drug_pval = nominal_corrs(dataset, demographic_columns, drugs_columns) demo_drug_corr_table . Alcohol consumption Amphetamines consumption Amyl nitrite consumption Benzodiazepine consumption Caffeine consumption Cannabis consumption Chocolate consumption Cocaine consumption Crack consumption Ecstasy consumption Heroin consumption Ketamine consumption Legal highs consumption Lysergic acid diethylamide consumption Methadone consumption Magic mushrooms consumption Nicotine consumption Volatile substance abuse consumption . Age 0.083 *** | 0.196 *** | 0.151 *** | 0.144 *** | 0.093 *** | 0.24 *** | 0.087 *** | 0.173 *** | 0.079 *** | 0.203 *** | 0.11 *** | 0.131 *** | 0.215 *** | 0.233 *** | 0.117 *** | 0.221 *** | 0.186 *** | 0.156 *** | . Gender 0.041 | 0.249 *** | 0.157 *** | 0.156 *** | 0.0 | 0.309 *** | 0.088 ** | 0.178 *** | 0.151 *** | 0.241 *** | 0.131 *** | 0.189 *** | 0.327 *** | 0.286 *** | 0.188 *** | 0.278 *** | 0.193 *** | 0.139 *** | . Education 0.079 *** | 0.122 *** | 0.045 * | 0.093 *** | 0.033 | 0.166 *** | 0.043 * | 0.068 *** | 0.082 *** | 0.125 *** | 0.068 *** | 0.061 *** | 0.144 *** | 0.133 *** | 0.106 *** | 0.14 *** | 0.131 *** | 0.098 *** | . Country 0.07 *** | 0.184 *** | 0.112 *** | 0.171 *** | 0.044 * | 0.229 *** | 0.063 *** | 0.123 *** | 0.117 *** | 0.146 *** | 0.136 *** | 0.061 *** | 0.183 *** | 0.213 *** | 0.189 *** | 0.203 *** | 0.136 *** | 0.151 *** | . Ethnicity 0.148 *** | 0.047 ** | 0.035 . | 0.046 ** | 0.072 *** | 0.088 *** | 0.023 | 0.047 ** | 0.052 ** | 0.029 | 0.0 | 0.0 | 0.049 ** | 0.062 *** | 0.022 | 0.056 *** | 0.063 *** | 0.074 *** | . demo_drug_corr_table_no_pvalues, _ = nominal_corrs(dataset, demographic_columns, drugs_columns, pvalues=False) heatmap_corr(demo_drug_corr_table_no_pvalues, ready=True, mask=False, nominal=True); . Ethnicity isn&#39;t correlated to drug taking, except very weakly with alcohol consumption. . Education also seems to be at most very weakly correlated to drug consumption. . Caffeine and Chocolate have no correlations with demographic variables. . There are no strong or even moderate correlations, the strongest weak correlations are mostly found in Gender. . Underneath is a list of the significant correlations with (|r| &gt;= 0.2). . get_ranked_corr(demo_drug_corr_table_no_pvalues, demo_drug_pval, corr_threshold=0.2) . Correlation . Gender / Legal highs consumption 0.327 | . Gender / Cannabis consumption 0.309 | . Gender / Lysergic acid diethylamide consumption 0.286 | . Gender / Magic mushrooms consumption 0.278 | . Gender / Ecstasy consumption 0.241 | . Age / Cannabis consumption 0.240 | . Age / Lysergic acid diethylamide consumption 0.233 | . Country / Cannabis consumption 0.229 | . Age / Magic mushrooms consumption 0.221 | . Age / Legal highs consumption 0.215 | . Country / Lysergic acid diethylamide consumption 0.213 | . Age / Ecstasy consumption 0.203 | . Country / Magic mushrooms consumption 0.203 | . Let&#39;s look at the consumption median of cannabis and legal highs consumption for female and male, which are the two drugs most correlated with gender according to our above table. . The significance is base on the Mann-Whitney U test (used in the below function). . &quot;In statistics, the Mann–Whitney U test is a nonparametric test of the null hypothesis that, for randomly selected values X and Y from two populations, the probability of X being greater than Y is equal to the probability of Y being greater than X&quot;(Wikipedia) . We implemented the difference test in the above procedure. . Here the two populations are male and female. . def diff_test(dataset, sample_attr, diff_attr, sample_attr_split=&#39;median&#39;, diff_attr_type=&#39;ordered&#39;, split_point=None, alpha=0.05): &#39;&#39;&#39; Difference significance test on dataset. Returns (p of no difference, the group split point, the group 1 central, the group 2 central, a summary). &lt;- shouldn&#39;t be in here but whatever sample_attr - column which will be divided into two samples with median value; diff_attr - attribute, which value will be checked in two sample groups; diff_attr_type - determines type of data which medians will be compared (ordered, interval) &#39;&#39;&#39; # In case of big data imbalance in highest rank (contains more than half) # It would make an empty group when splitting futher down if split_point is None and sample_attr_split == &quot;median&quot;: levels = dataset[sample_attr].unique() if not (levels &gt; dataset[sample_attr].median()).any(): # The highest level is the same as median split_point = levels[-1] # Split one rank lower instead if split_point is None: split_point = dataset[sample_attr].median() if sample_attr_split==&#39;median&#39; else dataset[sample_attr].mean() group_1 = dataset[dataset[sample_attr] &gt; split_point] group_2 = dataset[dataset[sample_attr] &lt;= split_point] group_1_diff_attr_central = group_1[diff_attr].median() if diff_attr_type==&#39;ordered&#39; else group_1[diff_attr].mean() group_2_diff_attr_central = group_2[diff_attr].median() if diff_attr_type==&#39;ordered&#39; else group_2[diff_attr].mean() if diff_attr_type == &#39;ordered&#39;: diff_sign, p = mannwhitneyu(group_1[diff_attr], group_2[diff_attr]) elif diff_attr_type == &#39;interval&#39;: diff_sign, p = ttest_ind(group_1[diff_attr], group_2[diff_attr]) else: raise ValueError(&#39;diff_attr_type should be one of ordered and interval&#39;) are = &#39;are&#39; if p &lt; alpha else &#39;are not&#39; sample_central = &#39;split point&#39; if split_point is not None else sample_attr_split diff_central = &#39;median&#39; if diff_attr_type==&#39;ordered&#39; else &#39;mean&#39; return (p, split_point, group_1_diff_attr_central, group_2_diff_attr_central, f&#39;First group: {sample_attr} above {sample_central} value {round(split_point, 3)} n Second group: {sample_attr} equal or below {sample_central} value {round(split_point, 6)} n First group {diff_attr} {diff_central}: {round(group_1_diff_attr_central, 3)} n Second group {diff_attr} {diff_central}: {round(group_2_diff_attr_central, 3)} n Difference significance for samples: {round(diff_sign, 3)} with p-value: {round(p, 3)} n Samples {are} statistically different.&#39;) . val_between_male_female = (dataset[&#39;Gender&#39;][demo_data[&#39;Gender&#39;] == &#39;Male&#39;].iloc[0] + dataset[&#39;Gender&#39;][demo_data[&#39;Gender&#39;] == &#39;Female&#39;].iloc[0])/ 2 . print(diff_test(dataset, &#39;Gender&#39;, &#39;Cannabis consumption&#39;, split_point=val_between_male_female)[-1]) . First group: Gender above split point value 0.0 Second group: Gender equal or below split point value 0.0 First group Cannabis consumption median: 2.0 Second group Cannabis consumption median: 4.0 Difference significance for samples: 288686.5 with p-value: 0.0 Samples are statistically different. . print(diff_test(dataset, &#39;Gender&#39;, &#39;Legal highs consumption&#39;, split_point=val_between_male_female)[-1]) . First group: Gender above split point value 0.0 Second group: Gender equal or below split point value 0.0 First group Legal highs consumption median: 0.0 Second group Legal highs consumption median: 2.0 Difference significance for samples: 290886.5 with p-value: 0.0 Samples are statistically different. . Females (first group) consume less cannabis and legal highs than men. . As a reminder, here are the different levels for each drug: . 0 - Never Used (median female legal highs consumer) . 1 - Used over a Decade Ago . 2 - Used in Last Decade (median female cannabis consumer, median male legal high consumer) . 3 - Used in Last Year . 4 - Used in Last Month (median male cannabis consumer) . 5 - Used in Last Week . 6 - Used in Last Day . And the values for male and female, respectively, are: . (dataset[&#39;Gender&#39;][demo_data[&#39;Gender&#39;] == &#39;Male&#39;].iloc[0], dataset[&#39;Gender&#39;][demo_data[&#39;Gender&#39;] == &#39;Female&#39;].iloc[0]) . (-0.48246, 0.48246) . Nominal correlations between demographic info and personality traits (using Cram&#233;r&#39;s V) . demo_psycho_corr_table, demo_psycho_pvalues = nominal_corrs(dataset, demographic_columns, personality_columns) demo_psycho_corr_table . Neuroticism Extraversion Openness to experience Agreeableness Conscientiousness Impulsiveness Sensation seeking . Age 0.033 | 0.0 | 0.139 *** | 0.0 | 0.095 *** | 0.103 *** | 0.159 *** | . Gender 0.089 . | 0.063 | 0.139 *** | 0.22 *** | 0.173 *** | 0.19 *** | 0.261 *** | . Education 0.054 . | 0.032 | 0.094 *** | 0.0 | 0.112 *** | 0.072 *** | 0.096 *** | . Country 0.047 | 0.086 *** | 0.141 *** | 0.036 | 0.105 *** | 0.098 *** | 0.144 *** | . Ethnicity 0.0 | 0.0 | 0.04 | 0.0 | 0.0 | 0.045 * | 0.046 * | . demo_psycho_corr_table_no_pvalues, _ = nominal_corrs(dataset, demographic_columns, personality_columns, pvalues=False) heatmap_corr(demo_psycho_corr_table_no_pvalues, ready=True, nominal=True, mask=False); . Neuroticism has no statistically significant correlation with any demographic variable. . Extraversion has only one statistically significant correlation, with Country, which is non existent. . Education and Ethnicity has no correlation with any personality trait (too low or unsignificant). . Gender seems to have the strongest (least weak) correlations. . There are no moderate or strong correlations, only pretty weak ones at most. . Let&#39;s list the weak correlations (|r| &gt; 0.15) . get_ranked_corr(demo_psycho_corr_table_no_pvalues, demo_psycho_pvalues, corr_threshold=0.15) . Correlation . Gender / Sensation seeking 0.261 | . Gender / Agreeableness 0.220 | . Gender / Impulsiveness 0.190 | . Gender / Conscientiousness 0.173 | . Age / Sensation seeking 0.159 | . Let&#39;s display the median of Agreeableness and Sensation seeking for each gender. . print(diff_test(dataset, &#39;Gender&#39;, &#39;Agreeableness&#39;, split_point=val_between_male_female)[-1]) . First group: Gender above split point value 0.0 Second group: Gender equal or below split point value 0.0 First group Agreeableness median: 0.288 Second group Agreeableness median: -0.302 Difference significance for samples: 327571.0 with p-value: 0.0 Samples are statistically different. . print(diff_test(dataset, &#39;Gender&#39;, &#39;Sensation seeking&#39;, split_point=val_between_male_female)[-1]) . First group: Gender above split point value 0.0 Second group: Gender equal or below split point value 0.0 First group Sensation seeking median: -0.216 Second group Sensation seeking median: 0.401 Difference significance for samples: 312536.0 with p-value: 0.0 Samples are statistically different. . Women tend to be more agreeable and less sensation seeking than men. . Difference significance tests . In this section, we will split the population in 2 groups based on a feature. . All the features are rank based, we will use the Mann–Whitney U test against a target, to see if the target shows a statistically significant difference in median for the two groups. . Important: The test can show significant difference when the median values are the same. The significance means &quot;for randomly selected values X and Y from two populations, the probability of X being greater than Y is not equal to the probability of Y being greater than X.&quot; . The splitting point will be the median, except if the median is the highest rank the feature can have. In this case we split one rank lower. . Group 1: respondents &gt; split. | Group 2: respondents &lt;= split. | . In the following section, we will only show significant difference in median value for a given target (and only this value differs between the two groups). . Note: The below tables will have some Nan values. This is because the cell has no significant difference but the column is needed for other rows. . We won&#39;t comment on them but feel free to explore. The tables are quite self-explanatory. . def get_significant_diff_tests(dataset, samples, diff_attrs, alpha=0.05, display_same_center=False): &#39;&#39;&#39; Perform difference tests on several splitting attributes for different targets. &#39;&#39;&#39; samp_dfs = [] for sample in samples: diff_dfs = [] for diff_attr in diff_attrs: levels = dataset[sample].unique() split = None if len(levels) == 2: split = (dataset[sample][dataset[sample] == levels[0]].iloc[0] + dataset[sample][dataset[sample] == levels[1]].iloc[0])/ 2 p, split_point, group_1_diff_attr_central, group_2_diff_attr_central, _ = diff_test(dataset, sample, diff_attr, split_point=split, alpha=alpha) if p &lt; alpha and (group_1_diff_attr_central != group_2_diff_attr_central or display_same_center): split_point = round(split_point, 5) diff_dfs.append(pd.DataFrame({&#39;Median&#39;: [group_1_diff_attr_central, group_2_diff_attr_central], &#39;Pval / significant diff ?&#39;: [p, &quot;Yes&quot; if p &lt; alpha else &quot;No&quot;]}, index=[f&quot;&gt; {split_point}&quot;, f&quot;&lt;= {split_point}&quot;])) if diff_dfs: samp_dfs.append(pd.concat(diff_dfs, axis=1, keys=diff_attrs)) return pd.concat(samp_dfs, axis=0, keys=samples) if samp_dfs else None . Difference on drug consumption splitting on demographic . get_significant_diff_tests(dataset, demographic_columns, drugs_columns) . Alcohol consumption Amphetamines consumption Amyl nitrite consumption Benzodiazepine consumption Caffeine consumption Cannabis consumption Chocolate consumption Cocaine consumption Crack consumption Ecstasy consumption . Median Pval / significant diff ? Median Pval / significant diff ? Median Pval / significant diff ? Median Pval / significant diff ? Median Pval / significant diff ? Median Pval / significant diff ? Median Pval / significant diff ? Median Pval / significant diff ? Median Pval / significant diff ? Median Pval / significant diff ? . Age &gt; -0.07854 0.0 | 0.000000 | 1.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.000000 | 2.0 | 0.000000 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . &lt;= -0.07854 1.0 | Yes | 4.0 | Yes | 2.0 | Yes | 2.0 | Yes | 2.0 | Yes | 4.0 | Yes | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . Gender &gt; 0.0 0.0 | 0.000000 | 0.0 | 0.000000 | 2.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.000000 | 2.0 | 0.000000 | NaN | NaN | . &lt;= 0.0 1.0 | Yes | 1.0 | Yes | 4.0 | Yes | 1.0 | Yes | 2.0 | Yes | 2.0 | Yes | 1.0 | Yes | 1.0 | Yes | 4.0 | Yes | NaN | NaN | . Education &gt; -0.05921 0.0 | 0.000000 | 0.0 | 0.000000 | 2.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.000000 | 2.0 | 0.000000 | NaN | NaN | NaN | NaN | NaN | NaN | . &lt;= -0.05921 1.0 | Yes | 1.0 | Yes | 4.0 | Yes | 1.0 | Yes | 2.0 | Yes | 1.0 | Yes | 5.0 | Yes | NaN | NaN | NaN | NaN | NaN | NaN | . Country &gt; -0.46841 0.0 | 0.000000 | 0.0 | 0.000000 | 2.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.000000 | 0.0 | 0.000000 | 2.0 | 0.0 | . &lt;= -0.46841 2.0 | Yes | 3.0 | Yes | 5.0 | Yes | 2.0 | Yes | 2.0 | Yes | 3.0 | Yes | 2.0 | Yes | 2.0 | Yes | 2.0 | Yes | 5.0 | Yes | . Ethnicity &gt; -0.31685 1.0 | 0.010986 | 2.0 | 0.000049 | 4.0 | 0.001836 | 0.5 | 0.009257 | 1.0 | 0.044104 | 1.0 | 0.011844 | 1.0 | 0.002797 | 1.0 | 0.011986 | 4.0 | 0.045376 | NaN | NaN | . &lt;= -0.31685 0.0 | Yes | 0.0 | Yes | 3.0 | Yes | 0.0 | Yes | 0.0 | Yes | 0.0 | Yes | 0.0 | Yes | 0.0 | Yes | 3.0 | Yes | NaN | NaN | . Difference on drug consumption splitting on personality traits . get_significant_diff_tests(dataset, personality_columns, drugs_columns) . Alcohol consumption Amphetamines consumption Amyl nitrite consumption Benzodiazepine consumption Caffeine consumption Cannabis consumption Chocolate consumption Cocaine consumption Crack consumption . Median Pval / significant diff ? Median Pval / significant diff ? Median Pval / significant diff ? Median Pval / significant diff ? Median Pval / significant diff ? Median Pval / significant diff ? Median Pval / significant diff ? Median Pval / significant diff ? Median Pval / significant diff ? . Neuroticism &gt; 0.04257 1.0 | 0.000000 | 2.0 | 0.0 | 3.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.000081 | 1.0 | 0.00054 | 4.0 | 0.000000 | NaN | NaN | NaN | NaN | . &lt;= 0.04257 0.0 | Yes | 0.0 | Yes | 2.0 | Yes | 0.0 | Yes | 0.0 | Yes | 0.0 | Yes | 2.0 | Yes | NaN | NaN | NaN | NaN | . Extraversion &gt; -0.01928 1.0 | 0.000000 | 2.0 | 0.0 | 4.0 | 0.0 | 1.0 | 0.0 | 2.0 | 0.000000 | 2.0 | 0.00000 | 1.0 | 0.000000 | 2.0 | 0.0 | 4.0 | 0.0 | . &lt;= -0.01928 0.0 | Yes | 0.0 | Yes | 2.0 | Yes | 0.0 | Yes | 0.0 | Yes | 0.0 | Yes | 0.0 | Yes | 0.0 | Yes | 2.0 | Yes | . Openness to experience &gt; -0.01729 0.0 | 0.000001 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000004 | 0.0 | 0.00000 | 3.0 | 0.000068 | NaN | NaN | NaN | NaN | . &lt;= -0.01729 1.0 | Yes | 1.0 | Yes | 3.0 | Yes | 1.0 | Yes | 0.5 | Yes | 1.0 | Yes | 4.0 | Yes | NaN | NaN | NaN | NaN | . Agreeableness &gt; -0.00665 0.0 | 0.000000 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.00000 | 0.0 | 0.000000 | 0.0 | 0.0 | 2.0 | 0.0 | . &lt;= -0.00665 1.0 | Yes | 1.0 | Yes | 4.0 | Yes | 1.0 | Yes | 2.0 | Yes | 2.0 | Yes | 1.0 | Yes | 1.0 | Yes | 4.0 | Yes | . Conscientiousness &gt; -0.21712 1.0 | 0.000000 | 1.0 | 0.0 | 4.0 | 0.0 | 1.0 | 0.0 | 2.0 | 0.000000 | 2.0 | 0.00000 | 1.0 | 0.000000 | 1.0 | 0.0 | 4.0 | 0.0 | . &lt;= -0.21712 0.0 | Yes | 0.0 | Yes | 2.0 | Yes | 0.0 | Yes | 0.0 | Yes | 0.0 | Yes | 0.0 | Yes | 0.0 | Yes | 2.0 | Yes | . Impulsiveness &gt; 0.07987 2.0 | 0.000000 | 2.0 | 0.0 | 5.0 | 0.0 | 2.0 | 0.0 | 2.0 | 0.000000 | 2.0 | 0.00000 | 1.0 | 0.000000 | 2.0 | 0.0 | 5.0 | 0.0 | . &lt;= 0.07987 0.0 | Yes | 0.0 | Yes | 2.0 | Yes | 0.0 | Yes | 0.0 | Yes | 0.0 | Yes | 0.0 | Yes | 0.0 | Yes | 2.0 | Yes | . Difference on personality traits splitting on demographic . get_significant_diff_tests(dataset, demographic_columns, personality_columns) . Agreeableness Conscientiousness Extraversion Impulsiveness Neuroticism Openness to experience Sensation seeking . Median Pval / significant diff ? Median Pval / significant diff ? Median Pval / significant diff ? Median Pval / significant diff ? Median Pval / significant diff ? Median Pval / significant diff ? Median Pval / significant diff ? . Age &gt; -0.07854 -0.21712 | 0.000000 | -0.21575 | 0.0 | -0.247775 | 0.000000 | NaN | NaN | -0.14882 | 0.000000 | 0.25953 | 0.000000 | NaN | NaN | . &lt;= -0.07854 0.19268 | Yes | 0.40148 | Yes | 0.141430 | Yes | NaN | NaN | 0.13606 | Yes | -0.14277 | Yes | NaN | NaN | . Gender &gt; 0.0 0.28783 | 0.000000 | 0.25953 | 0.0 | 0.003320 | 0.002804 | -0.21712 | 0.0 | 0.04257 | 0.003225 | -0.17779 | 0.000000 | -0.21575 | 0.0 | . &lt;= 0.0 -0.30172 | Yes | -0.27607 | Yes | -0.154870 | Yes | 0.19268 | Yes | -0.05188 | Yes | 0.14143 | Yes | 0.40148 | Yes | . Education &gt; -0.05921 0.25953 | 0.000000 | -0.21712 | 0.0 | 0.167670 | 0.000017 | -0.21575 | 0.0 | -0.10035 | 0.000112 | 0.13136 | 0.000237 | NaN | NaN | . &lt;= -0.05921 -0.27607 | Yes | 0.19268 | Yes | -0.154870 | Yes | 0.07987 | Yes | 0.04257 | Yes | -0.01729 | Yes | NaN | NaN | . Country &gt; -0.46841 0.13136 | 0.000005 | 0.12331 | 0.0 | 0.003320 | 0.000025 | -0.21712 | 0.0 | -0.14882 | 0.000000 | -0.17779 | 0.000000 | -0.21575 | 0.0 | . &lt;= -0.46841 -0.15487 | Yes | -0.27607 | Yes | -0.154870 | Yes | 0.19268 | Yes | 0.31287 | Yes | 0.44585 | Yes | 0.40148 | Yes | . Ethnicity &gt; -0.31685 NaN | NaN | NaN | NaN | 0.529750 | 0.001346 | NaN | NaN | 0.14143 | 0.014002 | 0.40148 | 0.000401 | NaN | NaN | . &lt;= -0.31685 NaN | NaN | NaN | NaN | -0.217120 | Yes | NaN | NaN | -0.01928 | Yes | 0.07987 | Yes | NaN | NaN | . Consumer vs Non-consumer . In practice, we are more interested in predicting which drugs an individual user consumes and less in knowing how long it has been. . We want to regroup the different levels we currently have for each drug into two classes: User and Non-User. . Year and decade based split for binarization of targets . We are going to compare year and decade based splits. . Year based: Consumer = Used in last year | Decade based: Consumer = Used in last decade | . dataset_decade = dataset.copy() for i in drugs_columns: dataset_decade[i] = [&quot;Non Consumer&quot; if c &lt; 2 else &quot;Consumer&quot; for c in dataset[i]] dataset_year = dataset.copy() for i in drugs_columns: dataset_year[i] = [&quot;Non Consumer&quot; if c &lt; 3 else &quot;Consumer&quot; for c in dataset[i]] compare_year_decade = pd.concat([dataset_decade.assign(split=&quot;decade&quot;), dataset_year.assign(split=&quot;year&quot;)]) . plot(&quot;count&quot;, compare_year_decade, drugs_columns, fig_title=&quot;Count / Frequency consumer comparison between decade and year split n n n&quot;, rotate_label=False, add_args={&quot;hue&quot;: &quot;split&quot;, &quot;palette&quot;: &quot;Set2&quot;}) . . Note: Since we duplicated our original dataset, you can get the correct percentage for a split (decade or year) by doubling the value. This will also double the gap between the two splits ! Appart from Alcohol, Caffeine, Cannabis and Chocolate consumption (it doesn&#39;t change that much anyways for these), the decade based split is more balanced. . We could choose the decade based split for model training and inference because of the balance. . Also, someone may not have consumed a particular drug for a year because he didn&#39;t get the opportunity to or tried out other drugs during this time period but could take it again in the future. . Whereas, if he didn&#39;t consume it in the last 10 years, he probably doesn&#39;t plan to consume it again. . This rule can&#39;t really be applied to 18-24 year olds. . We will continue our analysis considering both splits. . Data analysis . grouped_df_decade = pd.DataFrame() for drug in drugs_columns: grouped_df_decade = pd.concat([grouped_df_decade, dataset_decade.groupby(drug).size()], axis=1) grouped_df_decade.columns = drugs_columns grouped_df_year = pd.DataFrame() for drug in drugs_columns: grouped_df_year = pd.concat([grouped_df_year, dataset_year.groupby(drug).size()], axis=1) grouped_df_year.columns = drugs_columns . grouped_df_decade.T.plot.bar(color=[&#39;red&#39;, &#39;green&#39;], figsize=(15,7), title=&quot;Count plot of consumer / non consumer for every drug&quot;); . Decade based split | . The consumers (red) are in a huge majority for Alcohol, Caffeine and Chocolate. . They are also around double the number of non-consumers for Cannabis and Nicotine. This may be due to the fact that lots of people try these out at least once, and the last time happened to be in the last decade. . For the other drugs, non-consumers are the majority. . Cocaine, Benzodiazepine, Amphetamines, Ecstasy, Legal highs and Magic mushrooms consumers account for more than half the non-consumers. . grouped_df_year.T.plot.bar(color=[&#39;red&#39;, &#39;green&#39;], figsize=(15,7), title=&quot;Count plot of consumer / non consumer for every drug&quot;); . Year based split | . The huge majorities for consumers are the same. . Cannabis and Nicotine still have a majority of consumers. Our previous hypothesis wasn&#39;t as relevant as I had expected. . The drugs cited previously for which the non-consumers accounted over half the consumers are consumed in a lesser proportion, but the scale of the decrease wasn&#39;t the same for all. . For some drug the proportions didn&#39;t change much, meaning people either consume them (on a yearly basis or more frequently) or not at all. . Let&#39;s encode drug users as 1 and non-users as 0. . for i in drugs_columns: dataset_decade[i] = [0 if c &lt; 2 else 1 for c in dataset[i]] for i in drugs_columns: dataset_year[i] = [0 if c &lt; 3 else 1 for c in dataset[i]] compare_year_decade = pd.concat([dataset_decade.assign(split=&quot;decade&quot;), dataset_year.assign(split=&quot;year&quot;)]) . Let&#39;s see the count of individual per number of drug used. . np_drugs = np.array([drug.split(&#39; &#39;, -1)[0] for drug in drugs_columns]) combinations = compare_year_decade[drugs_columns].apply( lambda x: pd.Series({ &#39;names&#39;:&#39;/&#39;.join(np_drugs[x.values.astype(bool)]), &#39;index&#39;: np.where(x.values)[0], &#39;number of drugs&#39;: x.values.sum()}), axis=1) combinations[&#39;split&#39;] = compare_year_decade[&#39;split&#39;] year_split = combinations[&#39;split&#39;] == &quot;year&quot; combinations.head() . names index number of drugs split . ID . 1 Alcohol/Amphetamines/Benzodiazepine/Caffeine/C... | [0, 1, 3, 4, 6, 16] | 6 | decade | . 2 Alcohol/Amphetamines/Amyl/Caffeine/Cannabis/Ch... | [0, 1, 2, 4, 5, 6, 7, 9, 11, 13, 14, 16] | 12 | decade | . 3 Alcohol/Caffeine/Cannabis/Chocolate | [0, 4, 5, 6] | 4 | decade | . 4 Alcohol/Benzodiazepine/Caffeine/Cannabis/Choco... | [0, 3, 4, 5, 6, 7, 11, 16] | 8 | decade | . 5 Alcohol/Caffeine/Cannabis/Chocolate/Magic/Nico... | [0, 4, 5, 6, 15, 16] | 6 | decade | . combinations[~year_split].describe() . number of drugs . count 1877.000000 | . mean 7.785296 | . std 4.212409 | . min 0.000000 | . 25% 4.000000 | . 50% 7.000000 | . 75% 11.000000 | . max 18.000000 | . For the decade split, the mean number of drug consumed is a bit lower than 8. Over 25% of the respondents are users of 11 drugs or more. The median is of 7 drugs. The mean is slightly higher than the median, meaning the distribution is slightly positively skewed. The number of drugs used ranges from 0 to 18 (all of them). . Note: The one(s) that consumed all of them probably didn&#8217;t lie or they would have lied for the Semeron. . . Note: The median and 3rd quartile number of drugs seem very high to me. I feel like this isn&#8217;t representative of the real life population (in the countries considered here), but maybe I am just no fun at parties :cry:. . combinations[year_split].describe() . number of drugs . count 1877.000000 | . mean 6.200852 | . std 3.468506 | . min 0.000000 | . 25% 3.000000 | . 50% 5.000000 | . 75% 9.000000 | . max 17.000000 | . For the year split, the mean number of drug consumed is a bit above 6. Over 25% of the respondents are users of 9 drugs or more. The median is of 5 drugs. The mean is higher than the median, meaning the distribution is positively skewed. The number of drugs used ranges from 0 to 17. None one consumed all the drugs in the last year. . catplot(combinations, &#39;number of drugs&#39;, hue=None, col=&#39;split&#39;, rotate_label=False, palette=&quot;Set2&quot;) . Decade split | . Barely anyone used 0, 1, 2, 17 or all 18 drugs. The largest number used are 3, 4, 5 and 6. . An interesting fact is that 10 has more respondents than 7, 8 and 9. I guess for some drugs, there is a threshold. Once people go over 6 drugs, they may tend to try out a bunch more. . Year split | . Again, barely anyone used 0, 1 and over 15 drugs. . The number of respondents decreases monotonically as the number of drugs increases (&gt;= 3 drugs). We dont see the same bump around 10, strengthening my previous hypothesis. . 3 drugs is now the huge majority! . Let&#39;s see which are the most common combinations! . value_counts_percentage(combinations[~year_split], &#39;names&#39;).head(10) . N % . Alcohol/Caffeine/Chocolate 290 | 15.45 | . Alcohol/Caffeine/Chocolate/Nicotine 122 | 6.50 | . Alcohol/Caffeine/Cannabis/Chocolate/Nicotine 100 | 5.33 | . Alcohol/Benzodiazepine/Caffeine/Chocolate 42 | 2.24 | . Alcohol/Caffeine/Cannabis/Chocolate 41 | 2.18 | . Alcohol/Benzodiazepine/Caffeine/Chocolate/Nicotine 23 | 1.23 | . Alcohol/Benzodiazepine/Caffeine/Cannabis/Chocolate/Nicotine 20 | 1.07 | . Caffeine/Chocolate 18 | 0.96 | . Alcohol/Amyl/Caffeine/Cannabis/Chocolate/Nicotine 16 | 0.85 | . Alcohol/Caffeine/Cannabis/Chocolate/Legal/Nicotine 15 | 0.80 | . value_counts_percentage(combinations[year_split], &#39;names&#39;).head(10) . N % . Alcohol/Caffeine/Chocolate 453 | 24.13 | . Alcohol/Caffeine/Chocolate/Nicotine 148 | 7.88 | . Alcohol/Caffeine/Cannabis/Chocolate/Nicotine 84 | 4.48 | . Caffeine/Chocolate 38 | 2.02 | . Alcohol/Benzodiazepine/Caffeine/Chocolate 34 | 1.81 | . Alcohol/Caffeine/Cannabis/Chocolate 31 | 1.65 | . Alcohol/Chocolate 23 | 1.23 | . Alcohol/Caffeine/Cannabis/Chocolate/Legal/Nicotine 22 | 1.17 | . Alcohol/Benzodiazepine/Caffeine/Cannabis/Chocolate/Nicotine 21 | 1.12 | . Alcohol/Benzodiazepine/Caffeine/Chocolate/Nicotine 19 | 1.01 | . No wonder the number of users of 3, 4 or 5 drugs were so high! . But really, we are not that interested in Chocolate, Caffeine, Nicotine and Alcohol. . Let&#39;s redo all this by not taking them into account. . np_drugs_illegal = np.array([drug.split(&#39; &#39;, -1)[0] for drug in drugs_illegal]) combinations_illegal = compare_year_decade[drugs_illegal].apply( lambda x: pd.Series({ &#39;names&#39;:&#39;/&#39;.join(np_drugs_illegal[x.values.astype(bool)]), &#39;index illegal&#39;: np.where(x.values)[0], &#39;number of illegal drugs&#39;: x.values.sum()}), axis=1) combinations_illegal[&#39;split&#39;] = compare_year_decade[&#39;split&#39;] . catplot(combinations_illegal, &quot;number of illegal drugs&quot;, hue=None, col=&quot;split&quot;, rotate_label=False, palette=&quot;Set2&quot;) . Removing legal drugs shifted everything to the left (basically fusing 0, 1, 2 and 3). The shape of the distribution remains around the same. . value_counts_percentage(combinations_illegal[~year_split], &#39;names&#39;).head(10) . N % . 467 | 24.88 | . Cannabis 149 | 7.94 | . Benzodiazepine 74 | 3.94 | . Benzodiazepine/Cannabis 30 | 1.60 | . Cannabis/Legal 19 | 1.01 | . Amphetamines/Amyl/Benzodiazepine/Cannabis/Cocaine/Ecstasy/Ketamine/Legal/Lysergic/Magic 19 | 1.01 | . Amyl/Cannabis 18 | 0.96 | . Cannabis/Legal/Magic 15 | 0.80 | . Cannabis/Cocaine 13 | 0.69 | . Cannabis/Ecstasy/Legal/Lysergic/Magic 11 | 0.59 | . value_counts_percentage(combinations_illegal[year_split], &#39;names&#39;).head(10) . N % . 706 | 37.61 | . Cannabis 134 | 7.14 | . Benzodiazepine 61 | 3.25 | . Benzodiazepine/Cannabis 32 | 1.70 | . Cannabis/Legal 30 | 1.60 | . Cannabis/Ecstasy 14 | 0.75 | . Cannabis/Magic 13 | 0.69 | . Cannabis/Ecstasy/Legal/Lysergic/Magic 12 | 0.64 | . Amphetamines/Cannabis 12 | 0.64 | . Cannabis/Ecstasy/Lysergic/Magic 12 | 0.64 | . A lot more people (37.61%) in the year split didn&#39;t consume any drug (not counting alcohol, caffeine, chocolate and nicotine) compared to the decade split (24.88%). . The first 4 rows (combination of drugs) are the same but then there is some variation (different order or different combinations). The proportions for each ranking are around the same for both splits, appart from the first row. . Cannabis comes second (with between 7% and 8% in both splits), as it is legal in some countries and a very popular drug. It is also often present in the popular tuples. . Let&#39;s redo it without cannabis. . drugs_illegal_no_cannabis = [drug for drug in drugs_illegal if drug != &quot;Cannabis consumption&quot;] np_drugs_illegal_no_cannabis = np.array([drug.split(&#39; &#39;, -1)[0] for drug in drugs_illegal_no_cannabis]) combinations_illegal_no_cannabis = compare_year_decade[drugs_illegal_no_cannabis].apply( lambda x: pd.Series({ &#39;names&#39;:&#39;/&#39;.join(np_drugs_illegal_no_cannabis[x.values.astype(bool)]), &#39;index illegal no cannabis&#39;: np.where(x.values)[0], &#39;number of illegal (no cannabis) drugs&#39;: x.values.sum()}), axis=1) combinations_illegal_no_cannabis[&#39;split&#39;] = compare_year_decade[&#39;split&#39;] catplot(combinations_illegal_no_cannabis, &quot;number of illegal (no cannabis) drugs&quot;, hue=None, col=&quot;split&quot;, rotate_label=False, palette=&quot;Set2&quot;) . The gap between 0 and 1 became even larger . value_counts_percentage(combinations_illegal_no_cannabis[~year_split], &#39;names&#39;).head(20) . N % . 616 | 32.82 | . Benzodiazepine 104 | 5.54 | . Amyl 24 | 1.28 | . Legal 22 | 1.17 | . Amphetamines/Amyl/Benzodiazepine/Cocaine/Ecstasy/Ketamine/Legal/Lysergic/Magic 19 | 1.01 | . Cocaine 18 | 0.96 | . Legal/Magic 16 | 0.85 | . Benzodiazepine/Methadone 13 | 0.69 | . Ecstasy/Legal/Lysergic/Magic 11 | 0.59 | . Amphetamines/Benzodiazepine/Cocaine/Crack/Ecstasy/Heroin/Ketamine/Legal/Lysergic/Methadone/Magic 11 | 0.59 | . Methadone 11 | 0.59 | . Magic 10 | 0.53 | . Volatile 10 | 0.53 | . Amphetamines/Benzodiazepine 9 | 0.48 | . Ecstasy 9 | 0.48 | . Amphetamines/Benzodiazepine/Ecstasy/Legal/Lysergic/Magic 9 | 0.48 | . Amphetamines/Cocaine/Ecstasy 9 | 0.48 | . Amphetamines/Benzodiazepine/Cocaine/Ecstasy/Ketamine/Legal/Lysergic/Methadone/Magic 9 | 0.48 | . Amphetamines/Amyl/Benzodiazepine/Cocaine/Crack/Ecstasy/Heroin/Ketamine/Legal/Lysergic/Methadone/Magic/Volatile 8 | 0.43 | . Amphetamines 8 | 0.43 | . value_counts_percentage(combinations_illegal_no_cannabis[year_split], &#39;names&#39;).head(20) . N % . 840 | 44.75 | . Benzodiazepine 93 | 4.95 | . Legal 35 | 1.86 | . Benzodiazepine/Methadone 18 | 0.96 | . Ecstasy 16 | 0.85 | . Methadone 15 | 0.80 | . Cocaine 14 | 0.75 | . Magic 13 | 0.69 | . Amphetamines 13 | 0.69 | . Ecstasy/Legal/Lysergic/Magic 12 | 0.64 | . Legal/Magic 12 | 0.64 | . Ecstasy/Lysergic/Magic 12 | 0.64 | . Lysergic/Magic 12 | 0.64 | . Amphetamines/Benzodiazepine 11 | 0.59 | . Legal/Lysergic/Magic 10 | 0.53 | . Cocaine/Ecstasy 10 | 0.53 | . Amphetamines/Benzodiazepine/Methadone 9 | 0.48 | . Cocaine/Ecstasy/Legal 9 | 0.48 | . Amphetamines/Benzodiazepine/Cocaine/Ecstasy/Ketamine/Legal/Lysergic/Magic 9 | 0.48 | . Ecstasy/Legal 8 | 0.43 | . Here we have the 20 most common nontrivial drugs combinations for both splits. . The decade split has more length combinations, meaning some respondents tried out several drugs in the last decade but didn&#39;t pick them all up for a more regular consumption. . compare_year_decade[drugs_columns].apply(lambda x: x[drugs_legal].sum() == 0 and x[drugs_illegal].sum() &gt; 0, axis=1).any() . False . I was curious as if any respondent could be an alcohol, caffeine, chocolate or nicotine non-user but a user of other drugs. Seems like not for both splits! . Conclusion . I hope you learnt lots reading this post. I certainly did! :smiley: . Warning: Don&#8217;t try out all the popular combination of drugs we just uncovered! . I plan on training a predictive model in a future post, so stay tuned. . Did I miss out on anything? Do you have any remark? Please let me know by annotating using Hypothesis or by commenting below! .",
            "url": "https://jonathan-sands.com/eda/tabular/multilabel/pandas/seaborn/matplotlib/2021/01/16/Drug-consumption-analysis.html",
            "relUrl": "/eda/tabular/multilabel/pandas/seaborn/matplotlib/2021/01/16/Drug-consumption-analysis.html",
            "date": " • Jan 16, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "An introduction to Pytorch and Fastai v2 on the MNIST dataset.",
            "content": "How ? . We will build a deep learning model for digit classification on the MNIST dataset using the Pytorch library first and then using the fastai library based on Pytorch to showcase how easy it makes building models. . Who does this blog post concern ? . This is addressed to people that have basic knowledge about deep learning and want to start building models. I will explain some aspects of deep learning but don&#39;t expect a full course starting scratch! . Type of model built . We won&#39;t create a brand new architecture for our neural net. Actually, in the first part using Pytorch, we will only include linear layers with some non-linearity between them. No convolution etc.. We aren&#39;t aiming at building a state of the art model. . Why ? . I made this as part of the homework recommendation from the Deep Learning for Coders with Fastai and PyTorch book I am currently reading. Go check it out ! . Downloading the data . The fastai library provides handy functionalities to download data and already has some urls for some famous datasets. . from fastai.vision.all import * import torchvision import torchvision.transforms as transforms from livelossplot import PlotLosses . URLs.MNIST . &#39;https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz&#39; . Using fatai&#39;s untar_data procedure, we will download and decompress the data from the above url in one go. The data will only be downloaded the first time. . Take a look at the documentation if you want to learn more. . path = untar_data(URLs.MNIST, dest=&quot;/workspace/data&quot;) Path.BASE_PATH = path path.ls() . (#2) [Path(&#39;testing&#39;),Path(&#39;training&#39;)] . As you can see, the data was already split into a training and testing dataset for our convenience! Let&#39;s take a peek into what is inside. . (path/&quot;training&quot;).ls() . (#10) [Path(&#39;training/1&#39;),Path(&#39;training/0&#39;),Path(&#39;training/5&#39;),Path(&#39;training/8&#39;),Path(&#39;training/7&#39;),Path(&#39;training/2&#39;),Path(&#39;training/3&#39;),Path(&#39;training/4&#39;),Path(&#39;training/9&#39;),Path(&#39;training/6&#39;)] . We have a different directory for every digit, each of them containing images (see below) of their corresponding digit. . This makes labeling easy. The label of each image is the name of its parent directory! . (path/&quot;training/1&quot;).ls() . (#6742) [Path(&#39;training/1/16455.png&#39;),Path(&#39;training/1/23960.png&#39;),Path(&#39;training/1/1816.png&#39;),Path(&#39;training/1/14821.png&#39;),Path(&#39;training/1/36273.png&#39;),Path(&#39;training/1/9842.png&#39;),Path(&#39;training/1/58767.png&#39;),Path(&#39;training/1/12793.png&#39;),Path(&#39;training/1/16887.png&#39;),Path(&#39;training/1/6530.png&#39;)...] . For example, the 1 directory contains 6742 images. One is displayed below. . image = Image.open((path/&quot;training/1&quot;).ls()[0]) image . image.size . (28, 28) . image.mode . &#39;L&#39; . This image and all the others in the data we just downloaded are 28x28 grayscale images (&#39;L&#39; mode means gray-scale). . The pytorch way . Data preparation . Making pytorch datasets . transform = transforms.Compose( [transforms.Grayscale(), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])] ) . Above are the transformations we will make to each of the images when creating our Pytorch datasets. . Step 1: Converting into a grayscale image, i.e. fusing the RGB color channels into a grayscale one (from what would be a [3, 28, 28] tensor to a [1, 28, 28]). . Tip: We need to do this because the loader parameter of ImageFolder (see next cell) loads 3 channels even if the original image only has one. I couldn&#8217;t bother creating a custom loader so this does the trick. . Step 2: Converting the grayscale image (with pixel values in the range [0, 255] into a 3 dimensional [1, 28, 28] pytorch tensor (with values in the range [0, 1]). . Step 3: We normalize with mean = 0.5 and std = 0.5 to get values from pixels in the range [-1, 1]. (pixel = (image - mean) / std maps 0 to -1 and 1 to 1). . Note: The argument of centering around 0 is usually held for activation functions inside the network (which we aren&#8217;t doing because we are using ReLU) but I did it for the input layer because I felt like it. This is not the same as standardizing but still gives a zero centered range. You can read this and the links given for more info. . full_dataset = torchvision.datasets.ImageFolder((path/&quot;training&quot;).as_posix(), transform = transform) # Splitting the above dataset into a training and validation dataset train_size = int(0.8 * len(full_dataset)) valid_size = len(full_dataset) - train_size training_set, validation_set = torch.utils.data.random_split(full_dataset, [train_size, valid_size]) # Dataset using the &quot;testing&quot; folder testing_set = torchvision.datasets.ImageFolder((path/&quot;testing&quot;).as_posix(), transform = transform) . We just built 3 datasets. A training dataset, a validation dataset and a testing dataset. . Our images in the &quot;training&quot; folders were divided randomnly into the testing and validation dataset with a ratio of 80% and 20% of the images respectively. . Testing dataset: Used to calculate our gradients and update our weights using the loss obtained forwarding the data through the network. . Validation dataset: Used to assess model performance on unseen data during the training. We tune our hyperparameters (learning rate, batch size, number of epochs, network structure etc.) to improve this performance. . Important: After some hyperparameter tuning, we may be satisfied with our model&#8217;s performance with the validation dataset. But the thing is, we tuned these hyperparameters to fit the validation data. The mesure of performance is biased because we adapted to the validation data. . Testing dataset: Used to get a final, unbiased, performance assessment. This data wasn&#39;t seen during the whole model building process. . Warning: You can&#8217;t go back and tune hyperparameters to improve this performance, because that would create the same problem as for the validation dataset. . From datasets to dataloaders . In pytorch, a &quot;Data loader combines a dataset and a sampler, and provides an iterable over the given dataset&quot;. Look at the documentation to learn more. . bs = 64 . The bs variable above corresponds to the batch size. This is the number of observations forwarded at a time in our neural network (and used to calculate our mean loss and then our gradients for the training). . Note: I chose batches of 64 because it seemed to work well for this application and my GPU isn&#8217;t great so I might run out of memory with anything larger. . train_loader = torch.utils.data.DataLoader(training_set, batch_size=bs, shuffle=True) validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=bs) dataloaders = { &quot;train&quot;: train_loader, &quot;validation&quot;: validation_loader } . We created a training and a testing data loader we will iterate on during our buidling process. The shuffle argument is set to True for the training data loader, meaning we will reshuffle the data at every epoch. . Training a neural network . Deep learning is like making a dish. I like to see the neural network&#39;s architecture as the plates / the cutlery / cooking tools, the weights as the ingredients and the hyperparameters as the cooking time / temperature / seasonning etc. . Creating the architecture . Without the proper tools, it would be impossible to make the dish you want and for it to be good, even if you found all the ingredients that satisfy your needs. . pytorch_net = nn.Sequential( nn.Flatten(), nn.Linear(28*28, 128), nn.ReLU(), nn.Linear(128, 50), nn.ReLU(), nn.Linear(50,10), nn.LogSoftmax(dim=1)) . Here we chose a simple but good enough network architecture. It may not be state of the art but as you will see, it still performs quite well! . Flatten: flattens our [1,28,28] tensor into a [1,784] tensor. Our model doesn&#39;t care if it was a square image to start with, it just sees numbers, and a&#39;s long as the same pixel in our original image gets mapped to the same input variable (one of the 784 values) each time, our model will be able to learn. We won&#39;t be doing any spatial treatment (like convolution , pooling etc.), so we just start be turning our input tensor into a feature vector that will be used by our classifier. . Linear: linear layer with an additive bias (bias parameter is set to True by default). . Important: This layer can only learn linear relations and stacking another one doesn&#8217;t change this fact since a single linear layer is capable of representing any consecutive number of linear layers. . ReLU: stands for Rectified linear unit is an activation function, also called a nonlinearity. It replaces every negative number with 0 (See plot below). By adding a nonlinear function between each linear layer, they become somewhat decoupled from each other and can each do its own useful work. Meaning with nonlinearity between linear layers we can now learn nonlinear relations! . LogSoftmax: applies log(Softmax(x)) to the last layer. Softmax maps all the values to [0, 1] and add up to 1 (probability distribution). log(Softmax) maps these values to [-inf, 0]. . Note: If you want to know why we use LogSoftmax instead of Softmax, you can read this. . Note: According to our above neural network structure, our first linear layer can construct 128 different features (each representing some different mix of pixels) and our second one (decoupled from the first) can learn 50! . device = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;) lr = 1e-2 nb_epoch = 77 . Before moving on, we should define a bunch of variables. &quot;A torch.device is an object representing the device on which a torch.Tensor is or will be allocated&quot;. Head here for more info. Here we want to perform our computations on a GPU if it is available. . lr is our learning rate hyperparameter representing the size of the step we take when applying SGD. . nb_epoch is our number of epochs, meaning the number of complete passes through the training dataset. . Note: I found the values for these hyperparameters to work well via trial and error. They are probably not optimal . optimizer = torch.optim.SGD(pytorch_net.parameters(), lr=lr) . The optimizer object above will handle the stochastic gradient descent (SGD) step for us. We need to pass it our model&#39;s parameters (so it can step on them) and a learning rate. . criterion = nn.NLLLoss() . We chose pytorch&#39;s nn.NLLLoss() for our loss function. It stands for negative log likelihood loss and is useful to train a classification problem with more than 2 classes. It expects log-probabilities as input for each class, which is our case after applying LogSoftmax. . Tip: Instead of applying a LogSoftmax layer in the last layer of our network and using NLLLoss, we could have used CrossEntropyLoss instead which is a loss that combines the two into one single class. Read the doc for more. . def train_model(model, criterion, optimizer, dataloaders, num_epochs=10): liveloss = PlotLosses() # Live training plot generic API model = model.to(device) # Moves and/or casts the parameters and buffers to device. for epoch in range(num_epochs): # Number of passes through the entire training &amp; validation datasets logs = {} for phase in [&#39;train&#39;, &#39;validation&#39;]: # First train, then validate if phase == &#39;train&#39;: model.train() # Set the module in training mode else: model.eval() # Set the module in evaluation mode running_loss = 0.0 # keep track of loss running_corrects = 0 # count of carrectly classified inputs for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) # Perform Tensor device conversion labels = labels.to(device) outputs = model(inputs) # forward pass through network loss = criterion(outputs, labels) # Calculate loss if phase == &#39;train&#39;: optimizer.zero_grad() # Set all previously calculated gradients to 0 loss.backward() # Calculate gradients optimizer.step() # Step on the weights using those gradient w -= gradient(w) * lr _, preds = torch.max(outputs, 1) # Get model&#39;s predictions running_loss += loss.detach() * inputs.size(0) # multiply mean loss by the number of elements running_corrects += torch.sum(preds == labels.data) # add number of correct predictions to total epoch_loss = running_loss / len(dataloaders[phase].dataset) # get the &quot;mean&quot; loss for the epoch epoch_acc = running_corrects.float() / len(dataloaders[phase].dataset) # Get proportion of correct predictions # Logging prefix = &#39;&#39; if phase == &#39;validation&#39;: prefix = &#39;val_&#39; logs[prefix + &#39;log loss&#39;] = epoch_loss.item() logs[prefix + &#39;accuracy&#39;] = epoch_acc.item() liveloss.update(logs) # Update logs liveloss.send() # draw, display stuff . We have our everything needed to cook our meal ! The actual cooking takes place in the above function, which handles the training phase and validation phase. . . The above graph illustrates what is going on during our training phase. We use our model to make predictions and calculate our loss (NLLLoss here) based on the real labels, then calulate the gradients using loss.backward() (computes dloss/dx for every parameter x which has requires_grad=True, which is the case for nn.Parameters() that we use under the hood) and step the weights with our optimizer before repeating the process. The stop condition in our case is just the number of epochs. . Note: You can use other stop conditions, such as a minimum accuracy to be obtained on the validation set etc. . The validation phase is basically the same process without calculating gradients and stepping since we are only intersted on measuring model performance. . Tip: Be sure to call model.train() and model.eval() before the corresponding phase. Some layers like BatchNorm and Dropout have a different behavior during training and evaluation. This is not the case of our model but it is still good habit) . train_model(pytorch_net, criterion, optimizer, dataloaders, nb_epoch) . accuracy training (min: 0.668, max: 0.998, cur: 0.998) validation (min: 0.850, max: 0.978, cur: 0.977) log loss training (min: 0.015, max: 1.224, cur: 0.015) validation (min: 0.080, max: 0.536, cur: 0.083) . After 80 epochs we get 97.7% accuracy on the validation data, which is very good for a simple model such as this one! . Even though our validation loss and accuracy stabilized themselves after around 50 epochs, I kept going for a couple epochs just in case I could squeeze a bit more out. . Note: Notice how training loss is close to 0 and accuracy nearly at 100%. This means our network nearly perfectly memorized the entire training data. . Tip: Plotting metrics such as accuracy and loss lets you visualize the process and helps with parameter tuning (what could be causing spikes? Is the accuracy going up / down or has it reached a plateau? etc.) . Warning: If validation loss started to go up and validation accuracy started to go down after some time, that would be signs of overfitting! Models using architectures with more layers take longer to train, and are more prone to overfitting. Training on small amount of data makes memorizing easier and can also lead to overfitting. Be careful! . torch.save(pytorch_net, &#39;models/pytorch-97.7acc.pt&#39;) . Let&#39;s save our trained model for inference using torch.save. . Warning: This saves the entire module using Python&#8217;s pickle module. There are disadvantages to this approach. I used it here because it is more intuitive and is not the focus of this blog post, but feel free to read this official pytorch document to learn more about use cases regarding the saving and loading of Pytorch models. . The Fastai way . &quot;fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches.&quot;. . Read the docs to learn more! . Data preparation . block = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, batch_tfms=aug_transforms(mult=2., do_flip=False)) . The DataBlock class is a &quot;generic container to quickly build Datasets and DataLoaders&quot;. . blocks: This is the way of telling the API that our inputs are images and our targets are categories. Types are represented by blocks, here we use ImageBlock and CategoryBlock for inputs and targets respectively. . | get_items: expects a function to assemble our items inside the data block. get_image_files searches subfolder for all image filenames recursively. . | splitter: Controls how our validation set is created. RandomSplitter splits items between training and validation (with valid_pct portion in validation) randomnly. . | get_y: expects a function to label data according to file name. parent_label labels items with the parent folder name. . | batch_tfms: These are transformations applied to batched data samples on the GPU. aug_transforms is an &quot;utility function to create a list of flip, rotate, zoom, warp and lighting transforms&quot;. (Here we disabled flipping because we don&#39;t want to train on mirrored images and use twice the amount of augmentation compared to the default.) These augmentations are only done on the training set, we don&#39;t want to evaluate our model&#39;s performance on distorted images. . | . . Note: &quot;Our model can learn to focus on, and recognize, different features in our images. It also reflects how images work in the real world: different photos of the same thing may be framed in slightly different ways.In fact, an entirely untrained neural network knows nothing whatsoever about how images behave. It doesn&#8217;t even recognize that when an object is rotated by one degree, it still is a picture of the same thing! So actually training the neural network with examples of images where the objects are in slightly different places and slightly different sizes helps it to understand the basic concept of what an object is, and how it can be represented in an image.&quot; (Deep Learning for Coders with Fastai and PyTorch) . This doesn&#39;t acutally build the datasets and data loaders since we didn&#39;t actually gave it our images yet. But once we do, it knows exactly how to deal with them! . loaders = block.dataloaders(path/&quot;training&quot;) loaders.train.show_batch(max_n=4, nrows=1) . block.dataloaders creates a DataLoaders object from the source we give it. Here we gave it the training folder. The sample of images shown are outputs from the created training data loader. As you can see, they are correctly labeled and quite distorted due to the batch augmentations we made. . We use the default value of 64 for our batch size (bs parameter). . Training a neural network . learn = cnn_learner(loaders, resnet34, metrics=accuracy) . cnn_learner builds a convolutional neural network style learner from dataloaders and an architecture. In our case we use the ResNet architecture. The 34 refers to the number of layers in this variant of the architecture. . cnn_learner has a parameter called pretrained which defaults to True, that sets the weights in our model to values already trained by experts to recognize thousands of categories on the ImageNet dataset. . When using a pretrained model, cnn_learner will remove the last layer since that is always specifically customized to the original training task (i.e. ImageNet dataset classification), and replace it with one or more new layers with randomized weights (called the head), of an appropriate size for the dataset you are working with. . Tip: &quot;You should nearly always use a pretrained model, because it means that your model, before you&#8217;ve even shown it any of your data, is already very capable. And, as you&#8217;ll see, in a deep learning model many of these capabilities are things you&#8217;ll need, almost regardless of the details of your project. For instance, parts of pretrained models will handle edge, gradient, and color detection, which are needed for many tasks.&quot; (Deep Learning for Coders with Fastai and PyTorch) . learn.lr_find() . SuggestedLRs(lr_min=0.012022644281387329, lr_steep=0.007585775572806597) . learn.lr_find() explores learning rates in a given range ([1e-7, 10] by default) over a number of iterations (100 default) and plots the loss versus the learning rates on a log scale. . Tip: A rule of thumb is choosing a value that is approximately in the middle of the sharpest downward slope (around 1e-2 here). You can also use the lr_min and lr_steep indicators above and choose a learning rate between them. . learn.fine_tune(12, base_lr=1e-2, cbs=[ShowGraphCallback()]) . epoch train_loss valid_loss accuracy time . 0 | 0.720954 | 0.305432 | 0.901917 | 01:00 | . epoch train_loss valid_loss accuracy time . 0 | 0.206855 | 0.076552 | 0.978250 | 01:24 | . 1 | 0.154104 | 0.069363 | 0.978500 | 01:23 | . 2 | 0.122340 | 0.059505 | 0.983250 | 01:23 | . 3 | 0.110277 | 0.044111 | 0.987417 | 01:23 | . 4 | 0.091840 | 0.049989 | 0.985250 | 01:23 | . 5 | 0.074792 | 0.027301 | 0.992833 | 01:23 | . 6 | 0.071336 | 0.025267 | 0.992167 | 01:23 | . 7 | 0.065276 | 0.027973 | 0.991917 | 01:23 | . 8 | 0.043457 | 0.026573 | 0.991833 | 01:25 | . 9 | 0.041340 | 0.019284 | 0.994500 | 01:25 | . 10 | 0.033921 | 0.019462 | 0.994083 | 01:28 | . 11 | 0.030882 | 0.019053 | 0.994500 | 01:27 | . learn.fine_tune: &quot;Fine tune with freeze for freeze_epochs then with unfreeze for epochs using discriminative LR&quot; (docs) . By default a pretrained Learner is in a frozen state, meaning that only the head of the model will train while the body stays frozen. . To resume, fine_tune trains the head (automatically added by cnn_learner with random weights) without the body for a few epochs (defaults to 1) and then unfreezes the Learner and trains the whole model for a number of epochs (here we chose 12) using discriminative learning rates (which means it applies different learning rates for different parts of the model). . cbs expects a list of callbacks. Here we passed ShowGraphCallback which updates a graph of training and validation loss (as seen above). . Note: Discriminative learning rate is preferred on pretrained models since the early layers are already trained and already recognize features useful to our specific model. This means we don&#8217;t need to train these layers &quot;as hard&quot; (updating weights by smaller steps), which is why we use a range of learning rates from smaller ones for early layers to larger ones for later layers. . CONGRATS! . After training our model for a while, we get around 99.5% accuracy on our validation set with minimal effort! . learn.export(&quot;models/fastai-99acc.pkl&quot;) . learn.export saves the definition of how to create our DataLoaders on top of saving the architecture and parameters of the model. Saving the Dataloaders allows us to transform the data for inference in the same manner as our validation set by default, so data augmentation will not be applied. . interp = ClassificationInterpretation.from_learner(learn) . ClassificationInterpretation.from_learner() constructs an ClassificationInterpretation object from a learner. It gives a handful of interpretation methods for classification models. . interp.plot_confusion_matrix() . The above confusion matrix helps us visualize where our model made mistakes. It like the most confused number were 0 with 6, 6 with 8, 5 with 3 and 7 with 2. . interp.plot_top_losses(10) . We can also visualize which images resulted in the largest loss. It seems like the upper-left 9 was mislabeled as a 3, so our network was right. For some of those, even humans could have mistaken them ! . Evaluating our model&#39;s inference on the testing dataset! . Pytorch model . def test_model(model, criterion, test_loader): model = model.to(device) # Moves and/or casts the parameters and buffers to device. test_loss = 0.0 # keep track of loss test_corrects = 0 # count of carrectly classified inputs with torch.no_grad(): # Disable gradient calculation for inputs, labels in test_loader: inputs = inputs.to(device) # Perform Tensor device conversion labels = labels.to(device) outputs = model(inputs) # forward pass through network loss = criterion(outputs, labels) # Calculate loss _, preds = torch.max(outputs, 1) test_loss += loss * inputs.size(0) # multiply mean loss by the number of elements test_corrects += torch.sum(preds == labels.data) # add number of correct predictions to total avg_loss = test_loss / len(test_loader.dataset) # get the &quot;mean&quot; loss for the epoch avg_acc = test_corrects.float() / len(test_loader.dataset) # Get proportion of correct predictions return avg_loss.item(), avg_acc.item() . Our testing procedure is basically the same as our validation phase from the training procedure apart from the absence of epochs. (To be expected since they serve the same purpose!) . We infer predictions from our inputs by batches, then calculate the loss from them (how &quot;far&quot; they were from the real labels) and record the loss and the number of correctly labeled inputs, before averaging it all at the end. . testing_loader = torch.utils.data.DataLoader(testing_set, batch_size=bs) . Creation of a testing DataLoader to be passed to our testing procedure. . pytorch_loss, pytorch_accuracy = test_model(pytorch_net, criterion, testing_loader) . def print_loss_acc(loss, acc): print(&quot;Loss : {:.6f}&quot;.format(loss)) print(&quot;Accuracy : {:.6f}&quot;.format(acc)) . print_loss_acc(pytorch_loss, pytorch_accuracy) . Loss : 0.079889 Accuracy : 0.977300 . The results on the testing data are approximately the same as on the validation set! . Fastai model . learn = load_learner(&#39;models/fastai-99acc.pkl&#39;) . test_dl = learn.dls.test_dl(get_image_files(path/&quot;testing&quot;), with_labels=True) . test_dl creates a test dataloader from test_items (list of image paths) using validation transforms of dls. We set with_labels to True because we want the labels of each image to check the inference accuracy of our model. . fastai_loss, fastai_accuracy = learn.validate(dl=test_dl) . learn.validate returns the calculated loss and the metrics of the model on the dl data loader. . print_loss_acc(fastai_loss, fastai_accuracy) . Loss : 0.014902 Accuracy : 0.995400 . Our loss and accuracy are slightly better than on our validation set! . . Note: A better loss and accuracy on the testng dataset may suggest that the makers of the MNIST database chose more of the hard and ambiguous images for training in order to make the trained models more robust. . And that&#39;s about it! Not so hard heh! We now have a two digits classification models ready to be used for inference! . How was it ? . This is one of my first blog posts and it took me some time. Any feedback is welcome! . Was the whole model building process easier than expected? . Would you have done some parts a different way? . Was my explaination any good? . Please feel free to comment or annotate the text directly using Hypothes.is if you spotted any errors or have any questions! .",
            "url": "https://jonathan-sands.com/deep%20learning/fastai/pytorch/vision/classifier/2020/11/15/MNIST.html",
            "relUrl": "/deep%20learning/fastai/pytorch/vision/classifier/2020/11/15/MNIST.html",
            "date": " • Nov 15, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Building a CNN recognizing furniture from different Louis periods",
            "content": "Context . This project was done after reading chapter 2 of fastai&#39;s book. It aims at classifying furniture from 4 different eras : . Louis XIII | Louis XIV | Louis XV | Louis XVI | . The idea came from a friend of mine telling me there once was an endless debate at lunch about which era a particular piece of furniture came from. . Content . This notebook will take you through my building and deployment process using the fastai library. If you don&#39;t care about this and just want to check out the final result, please head here. . Building the dataset . To build the dataset, I used fatkun batch download image chrome plugin. There are many other ways to scrap the web for images, but in my case it turns out typing Louis XIV furniture (or should I say mobilier Louis XIV in french since this is a french thing) on Google image gives me a bunch of furniture from all eras. I found at that pinterest has some pretty good collections of images (for example here for Louis XIV styled furniture). It turns out this plugin makes it easy to go on a page then download every images from it, including trash (but you have the ability to remove it before downloading). I recommend you to go on every website/page you need to build your data before downloading, since the plugin detects duplicates (also avoiding the same trash images time and time again). . I put each type of images in it&#39;s own folder as you can see. . path = Path(&#39;data/train&#39;) path.ls() . (#4) [Path(&#39;data/train/XV&#39;),Path(&#39;data/train/XIII&#39;),Path(&#39;data/train/XVI&#39;),Path(&#39;data/train/XIV&#39;)] . Cleaning up . Let&#39;s remove corrupted images before moving on. . fns = get_image_files(path) # gets all the image files recursively in path failed = verify_images(fns) failed . █ . (#0) [] . As you can see, our dataset doesn&#39;t have any, and I am not sure if it can happen while using this plugin. But if you ever do have some, execute the next step. . failed.map(Path.unlink) . (#0) [] . From Data to DataLoaders . You should really go an read the chapter from the book if you want to understand what I am doing. I am basically, copy pasting their stuff, even this title is the same ! . furniture = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=1), # Fixed seed for tuning hyperparameters get_y=parent_label, item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = furniture.dataloaders(path) . Let&#39;s take a look at some of does images ! . dls.valid.show_batch(max_n=4, nrows=1) . Okay, looking good ! But we have lots of data. . len(fns) . 2052 . And among this data are probably some mislabeled ones and some that aren&#39;t even furniture ! To help us out in our cleaning process, we will use a quickly trained model. . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time 0 1.752790 0.920795 0.319095 00:06 epoch train_loss valid_loss error_rate time 0 1.079975 0.690126 0.258794 00:07 1 0.904969 0.654678 0.211055 00:07 2 0.726501 0.621098 0.208543 00:07 3 0.610325 0.598395 0.203518 00:07 . Even if we quickly trained our model on a small architecture (resnet18), we can get an idea of what the model has trouble identifying by plotting a confusion matrix. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . █ . But we haven&#39;t cleaned anything yet so let&#39;s do that ! . cleaner = ImageClassifierCleaner(learn) cleaner . █ . for idx in cleaner.delete(): cleaner.fns[idx].unlink() # Delete files you selected as Delete . for idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat) # Change label from mislabeled images . I redid this whole procedure another time, just to be sure. And I probably should do it again. Unfortunately, I am no furniture expert and can&#39;t really relabel mislabeled data so I guess my dataset is full of it. . Training the actual model ! . Now that our dataset is clean (not really haha), we can start training our actual production model ! . path = Path(&#39;data/train&#39;) fns = get_image_files(path) len(fns) . 1992 . Our dataset is smaller now that we removed 60 images ! . furniture = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=1), # Fixed seed for tuning hyperparameters get_y=parent_label, item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) . dls = furniture.dataloaders(path,bs=32) . learn = cnn_learner(dls, resnet50, metrics=error_rate) learn.fine_tune(8, cbs=[ShowGraphCallback()]) . epoch train_loss valid_loss error_rate time . 0 | 1.424473 | 1.235989 | 0.351759 | 00:12 | . epoch train_loss valid_loss error_rate time . 0 | 0.890016 | 0.578133 | 0.193467 | 00:16 | . 1 | 0.728691 | 0.681202 | 0.178392 | 00:16 | . 2 | 0.672835 | 0.601211 | 0.180905 | 00:16 | . 3 | 0.510261 | 0.606285 | 0.175879 | 00:16 | . 4 | 0.341562 | 0.585303 | 0.160804 | 00:16 | . 5 | 0.211855 | 0.521299 | 0.153266 | 00:16 | . 6 | 0.148523 | 0.524159 | 0.135678 | 00:16 | . 7 | 0.126175 | 0.551581 | 0.135678 | 00:16 | . learn.save(&#39;resnet50-13_error_rate&#39;) . Path(&#39;models/resnet50-13_error_rate.pth&#39;) . We used transfer learning, using resnet50 as our pretrained model (resnet50 architecture with pretrained weights from the ImageNet dataset) and a batch size of 32 (since I am training on my personal GPU RTX2060 that couldn&#39;t handle a larger batch size). As you can see, our error_rate on the validation set is aroud 13%, which is not great, but considering our approximate dataset, is understandable. . Making a web app . We are going now to make a notebook app and deploy it. . First let us save our learn object. . learn.export() . path = Path() path.ls(file_exts = &#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . learn_inf = load_learner(path/&#39;export.pkl&#39;) . To make our app, we will use IPython widgets and Voilà. . btn_upload = widgets.FileUpload() out_pl = widgets.Output() lbl_pred = widgets.Label() btn_run = widgets.Button(description=&#39;Classify&#39;) . Let&#39;s define what uploading an image does. . def on_data_change(change): lbl_pred.value = &#39;&#39; img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . btn_upload.observe(on_data_change, names=[&#39;data&#39;]) . display(VBox([widgets.Label(&#39;Select your piece of Louis furniture !&#39;), btn_upload, out_pl, lbl_pred])) . Above is what the user sees when he launches our app. . When uploading an image, the whole thing looks like this ! . Now, we just have to deploy our app ! Voilà can transform you jupyter notebook into a simple working web app. Of course we aren&#39;t transforming this entire notebook but instead this one here. . We are deploying our app on Binder. Check out these documentations on how to use voilà with Binder. . And TADAAAA! That&#39;s all it took ! Check out the working web app . Conclusion . What do I take out of this small project ? . Things that worked great . training | building app | deploying app | . Things that didn&#39;t work great / were hard . getting good data (lots of mislabeled data) | cleaning it up (lots of out-of-domain data) | finding the right number of epoch felt a little random | . Things to look into: . Find a way to get cleaner data | Perhaps use a different slice than default for fine tuning | Make the app cleaner | .",
            "url": "https://jonathan-sands.com/jupyter/fastai/voila/cnn/deep%20learning/2020/09/23/Louis_CNN.html",
            "relUrl": "/jupyter/fastai/voila/cnn/deep%20learning/2020/09/23/Louis_CNN.html",
            "date": " • Sep 23, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "❓About this blog",
          "content": "Why ? . I created this blog after reading chapter 2 of fastai’s book, teaching deep learning with a top down approach, where they recommended starting a blog. . . I really recommend checking their tutorials and courses, they are great I don’t expect any clear content to start with. This is also a way to help me learn and get things sorted out inside my head. But hey, if you are reading this, you may be in the same situation and can get to learn along with me :wink:. . Content to expect . This will be a list of what I think will appear on this blog in the future, based on what I am currently doing / learning and what I want to learn in the future. . Machine learning Deep learning | deep reinforcement learning | generative modeling | others | . | Some maths1 | Rambling about topics such as: Rocket Science and other science mystery things (sounds scary) | AI | Engineering and technology stuff | . | . I recently ordered my first math book and I intend to learn and understand math more deeply &#8617; . |",
          "url": "https://jonathan-sands.com/about-blog/",
          "relUrl": "/about-blog/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "👱About Me",
          "content": ". Important: Download CV Hi ! My name is Jonathan SANDS, a french engineering student just starting a master’s program in AI. If you want to learn more about me then you are in the right place ! . . Note: If you want to know more about the content of this blog, please head here instead ! :wink: Quick overview of myself . 2016: French Baccalauréat, France’s national highschool exam. (17.9/20 average in Science, 15.5/20 average accounting all subjects) . | 2016-2019: Studied at EPITA, a french computer science and software engineering focused engineering school. . | 2018: I spent the first semester abroad in Beijing at BJTU, as part of an exchange agreement with my french school. . | 2019-now: Studying at ESILV, another french engineering school part of the “Pôle Universitaire Léonard de Vinci” (a group of several schools). I did my last year of undergraduate here, and just joined its master’s degree program in AI and data science. . | . . I transfered from EPITA to ESILV because EPITA was mostly about coding brainlessly, whereas ESILV has a more scientific based approach I like better 2020: During the first semester, I left for Romania to study at UBB, as part of an exchange program with my french school. | . . I was supposed to study a master&#39;s program in Control Science and Engineering at SJTU in Shanghai, as part of the UM-SJTU joint institute, starting 2020 for 2.5 years but I changed my mind because of the COVID-19 virus :cry: Interests . Techie / Science stuff Computer science | Rocket Science | Math | Everything new and exciting ! | . | Life stuff Travelling abroad / experiencing new cultures | Sports (volleyball, calisthenics, climbing, running, anything really :grin:) | Any new experience ! | . | . Medias . . Tip: Click on the logos to get to my various profiles ! . . . . . . There was a &#39;Forbidden&#39; error fetching URL: &#39;https://twitter.com/JonathanSands14/status/1309793203407802370&#39; . . . . Don&#39;t expect to see much here",
          "url": "https://jonathan-sands.com/about-me/",
          "relUrl": "/about-me/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jonathan-sands.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}