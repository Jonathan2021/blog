{
  
    
        "post0": {
            "title": "An introduction to Pytorch and Fastai on the MNIST dataset.",
            "content": "How ? . We will build a deep learning model for digit classification on the MNIST dataset using the Pytorch library first and then using the fastai library based on Pytorch to showcase how easy it makes building models. . Who does this blog post concern ? . This is addressed to people that have basic knowledge about deep learning and want to start building models. I will explain some aspects of deep learning but don&#39;t expect a full course starting scratch! . Type of model built . We won&#39;t create a brand new architecture for our neural net. Actually, in the first part using Pytorch, we will only include linear layers with some non-linearity between them. No convolution etc.. We aren&#39;t aiming at building a state of the art model. . Why ? . I made this as part of the homework recommendation from the Deep Learning for Coders with Fastai and PyTorch book I am currently reading. Go check it out ! . Downloading the data . The fastai library provides handy functionalities to download data and already has some urls for some famous datasets. . from fastai.vision.all import * import torchvision import torchvision.transforms as transforms from livelossplot import PlotLosses . URLs.MNIST . &#39;https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz&#39; . Using fatai&#39;s untar_data procedure, we will download and decompress the data from the above url in one go. The data will only be downloaded the first time. . Take a look at the documentation if you want to learn more. . path = untar_data(URLs.MNIST, dest=&quot;/workspace/data&quot;) Path.BASE_PATH = path path.ls() . (#2) [Path(&#39;testing&#39;),Path(&#39;training&#39;)] . As you can see, the data was already split into a training and testing dataset for our convenience! Let&#39;s take a peek into what is inside. . (path/&quot;training&quot;).ls() . (#10) [Path(&#39;training/1&#39;),Path(&#39;training/0&#39;),Path(&#39;training/5&#39;),Path(&#39;training/8&#39;),Path(&#39;training/7&#39;),Path(&#39;training/2&#39;),Path(&#39;training/3&#39;),Path(&#39;training/4&#39;),Path(&#39;training/9&#39;),Path(&#39;training/6&#39;)] . We have a different directory for every digit, each of them containing images (see below) of their corresponding digit. . This makes labeling easy. The label of each image is the name of its parent directory! . (path/&quot;training/1&quot;).ls() . (#6742) [Path(&#39;training/1/16455.png&#39;),Path(&#39;training/1/23960.png&#39;),Path(&#39;training/1/1816.png&#39;),Path(&#39;training/1/14821.png&#39;),Path(&#39;training/1/36273.png&#39;),Path(&#39;training/1/9842.png&#39;),Path(&#39;training/1/58767.png&#39;),Path(&#39;training/1/12793.png&#39;),Path(&#39;training/1/16887.png&#39;),Path(&#39;training/1/6530.png&#39;)...] . For example, the 1 directory contains 6742 images. One is displayed below. . image = Image.open((path/&quot;training/1&quot;).ls()[0]) image . image.size . (28, 28) . image.mode . &#39;L&#39; . This image and all the others in the data we just downloaded are 28x28 grayscale images (&#39;L&#39; mode means gray-scale). . The pytorch way . Data preparation . Making pytorch datasets . transform = transforms.Compose( [transforms.Grayscale(), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])] ) . Above are the transformations we will make to each of the images when creating our Pytorch datasets. . Step 1: Converting into a grayscale image, i.e. fusing the RGB color channels into a grayscale one (from what would be a [3, 28, 28] tensor to a [1, 28, 28]). . Tip: We need to do this because the loader parameter of ImageFolder (see next cell) loads 3 channels even if the original image only has one. I couldn&#8217;t bother creating a custom loader so this does the trick. . Step 2: Converting the grayscale image (with pixel values in the range [0, 255] into a 3 dimensional [1, 28, 28] pytorch tensor (with values in the range [0, 1]). . Step 3: We normalize with mean = 0.5 and std = 0.5 to get values from pixels in the range [-1, 1]. (pixel = (image - mean) / std maps 0 to -1 and 1 to 1). . Note: The argument of centering around 0 is usually held for activation functions inside the network (which we aren&#8217;t doing because we are using ReLU) but I did it for the input layer because I felt like it. This is not the same as standardizing but still gives a zero centered range. You can read this and the links given for more info. . full_dataset = torchvision.datasets.ImageFolder((path/&quot;training&quot;).as_posix(), transform = transform) # Splitting the above dataset into a training and validation dataset train_size = int(0.8 * len(full_dataset)) valid_size = len(full_dataset) - train_size training_set, validation_set = torch.utils.data.random_split(full_dataset, [train_size, valid_size]) # Dataset using the &quot;testing&quot; folder testing_set = torchvision.datasets.ImageFolder((path/&quot;testing&quot;).as_posix(), transform = transform) . We just built 3 datasets. A training dataset, a validation dataset and a testing dataset. . Our images in the &quot;training&quot; folders were divided randomnly into the testing and validation dataset with a ratio of 80% and 20% of the images respectively. . Testing dataset: Used to calculate our gradients and update our weights using the loss obtained forwarding the data through the network. . Validation dataset: Used to assess model performance on unseen data during the training. We tune our hyperparameters (learning rate, batch size, number of epochs, network structure etc.) to improve this performance. . Important: After some hyperparameter tuning, we may be satisfied with our model&#8217;s performance with the validation dataset. But the thing is, we tuned these hyperparameters to fit the validation data. The mesure of performance is biased because we adapted to the validation data. . Testing dataset: Used to get a final, unbiased, performance assessment. This data wasn&#39;t seen during the whole model building process. . Warning: You can&#8217;t go back and tune hyperparameters to improve this performance, because that would create the same problem as for the validation dataset. . From datasets to dataloaders . In pytorch, a &quot;Data loader combines a dataset and a sampler, and provides an iterable over the given dataset&quot;. Look at the documentation to learn more. . bs = 64 . The bs variable above corresponds to the batch size. This is the number of observations forwarded at a time in our neural network (and used to calculate our mean loss and then our gradients for the training). . Note: I chose batches of 64 because it seemed to work well for this application and my GPU isn&#8217;t great so I might run out of memory with anything larger. . train_loader = torch.utils.data.DataLoader(training_set, batch_size=bs, shuffle=True) validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=bs) dataloaders = { &quot;train&quot;: train_loader, &quot;validation&quot;: validation_loader } . We created a training and a testing data loader we will iterate on during our buidling process. The shuffle argument is set to True for the training data loader, meaning we will reshuffle the data at every epoch. . Training a neural network . Deep learning is like making a dish. I like to see the neural network&#39;s architecture as the plates / the cutlery / cooking tools, the weights as the ingredients and the hyperparameters as the cooking time / temperature / seasonning etc. . Creating the architecture . Without the proper tools, it would be impossible to make the dish you want and for it to be good, even if you found all the ingredients that satisfy your needs. . pytorch_net = nn.Sequential( nn.Flatten(), nn.Linear(28*28, 128), nn.ReLU(), nn.Linear(128, 50), nn.ReLU(), nn.Linear(50,10), nn.LogSoftmax(dim=1)) . Here we chose a simple but good enough network architecture. It may not be state of the art but as you will see, it still performs quite well! . Flatten: flattens our [1,28,28] tensor into a [1,784] tensor. Our model doesn&#39;t care if it was a square image to start with, it just sees numbers, and a&#39;s long as the same pixel in our original image gets mapped to the same input variable (one of the 784 values) each time, our model will be able to learn. We won&#39;t be doing any spatial treatment (like convolution , pooling etc.), so we just start be turning our input tensor into a feature vector that will be used by our classifier. . Linear: linear layer with an additive bias (bias parameter is set to True by default). . Important: This layer can only learn linear relations and stacking another one doesn&#8217;t change this fact since a single linear layer is capable of representing any consecutive number of linear layers. . ReLU: stands for Rectified linear unit is an activation function, also called a nonlinearity. It replaces every negative number with 0 (See plot below). By adding a nonlinear function between each linear layer, they become somewhat decoupled from each other and can each do its own useful work. Meaning with nonlinearity between linear layers we can now learn nonlinear relations! . LogSoftmax: applies log(Softmax(x)) to the last layer. Softmax maps all the values to [0, 1] and add up to 1 (probability distribution). log(Softmax) maps these values to [-inf, 0]. . Note: If you want to know why we use LogSoftmax instead of Softmax, you can read this. . Note: According to our above neural network structure, our first linear layer can construct 128 different features (each representing some different mix of pixels) and our second one (decoupled from the first) can learn 50! . device = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;) lr = 1e-2 nb_epoch = 77 . Before moving on, we should define a bunch of variables. &quot;A torch.device is an object representing the device on which a torch.Tensor is or will be allocated&quot;. Head here for more info. Here we want to perform our computations on a GPU if it is available. . lr is our learning rate hyperparameter representing the size of the step we take when applying SGD. . nb_epoch is our number of epochs, meaning the number of complete passes through the training dataset. . Note: I found the values for these hyperparameters to work well via trial and error. They are probably not optimal . optimizer = torch.optim.SGD(pytorch_net.parameters(), lr=lr) . The optimizer object above will handle the stochastic gradient descent (SGD) step for us. We need to pass it our model&#39;s parameters (so it can step on them) and a learning rate. . criterion = nn.NLLLoss() . We chose pytorch&#39;s nn.NLLLoss() for our loss function. It stands for negative log likelihood loss and is useful to train a classification problem with more than 2 classes. It expects log-probabilities as input for each class, which is our case after applying LogSoftmax. . Tip: Instead of applying a LogSoftmax layer in the last layer of our network and using NLLLoss, we could have used CrossEntropyLoss instead which is a loss that combines the two into one single class. Read the doc for more. . def train_model(model, criterion, optimizer, dataloaders, num_epochs=10): liveloss = PlotLosses() # Live training plot generic API model = model.to(device) # Moves and/or casts the parameters and buffers to device. for epoch in range(num_epochs): # Number of passes through the entire training &amp; validation datasets logs = {} for phase in [&#39;train&#39;, &#39;validation&#39;]: # First train, then validate if phase == &#39;train&#39;: model.train() # Set the module in training mode else: model.eval() # Set the module in evaluation mode running_loss = 0.0 # keep track of loss running_corrects = 0 # count of carrectly classified inputs for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) # Perform Tensor device conversion labels = labels.to(device) outputs = model(inputs) # forward pass through network loss = criterion(outputs, labels) # Calculate loss if phase == &#39;train&#39;: optimizer.zero_grad() # Set all previously calculated gradients to 0 loss.backward() # Calculate gradients optimizer.step() # Step on the weights using those gradient w -= gradient(w) * lr _, preds = torch.max(outputs, 1) # Get model&#39;s predictions running_loss += loss.detach() * inputs.size(0) # multiply mean loss by the number of elements running_corrects += torch.sum(preds == labels.data) # add number of correct predictions to total epoch_loss = running_loss / len(dataloaders[phase].dataset) # get the &quot;mean&quot; loss for the epoch epoch_acc = running_corrects.float() / len(dataloaders[phase].dataset) # Get proportion of correct predictions # Logging prefix = &#39;&#39; if phase == &#39;validation&#39;: prefix = &#39;val_&#39; logs[prefix + &#39;log loss&#39;] = epoch_loss.item() logs[prefix + &#39;accuracy&#39;] = epoch_acc.item() liveloss.update(logs) # Update logs liveloss.send() # draw, display stuff . We have our everything needed to cook our meal ! The actual cooking takes place in the above function, which handles the training phase and validation phase. . . The above graph illustrates what is going on during our training phase. We use our model to make predictions and calculate our loss (NLLLoss here) based on the real labels, then calulate the gradients using loss.backward() (computes dloss/dx for every parameter x which has requires_grad=True, which is the case for nn.Parameters() that we use under the hood) and step the weights with our optimizer before repeating the process. The stop condition in our case is just the number of epochs. . Note: You can use other stop conditions, such as a minimum accuracy to be obtained on the validation set etc. . The validation phase is basically the same process without calculating gradients and stepping since we are only intersted on measuring model performance. . Tip: Be sure to call model.train() and model.eval() before the corresponding phase. Some layers like BatchNorm and Dropout have a different behavior during training and evaluation. This is not the case of our model but it is still good habit) . train_model(pytorch_net, criterion, optimizer, dataloaders, nb_epoch) . accuracy training (min: 0.668, max: 0.998, cur: 0.998) validation (min: 0.850, max: 0.978, cur: 0.977) log loss training (min: 0.015, max: 1.224, cur: 0.015) validation (min: 0.080, max: 0.536, cur: 0.083) . After 80 epochs we get 97.7% accuracy on the validation data, which is very good for a simple model such as this one! . Even though our validation loss and accuracy stabilized themselves after around 50 epochs, I kept going for a couple epochs just in case I could squeeze a bit more out. . Note: Notice how training loss is close to 0 and accuracy nearly at 100%. This means our network nearly perfectly memorized the entire training data. . Tip: Plotting metrics such as accuracy and loss lets you visualize the process and helps with parameter tuning (what could be causing spikes? Is the accuracy going up / down or has it reached a plateau? etc.) . Warning: If validation loss started to go up and validation accuracy started to go down after some time, that would be signs of overfitting! Models using architectures with more layers take longer to train, and are more prone to overfitting. Training on small amount of data makes memorizing easier and can also lead to overfitting. Be careful! . torch.save(pytorch_net, &#39;models/pytorch-97.7acc.pt&#39;) . Let&#39;s save our trained model for inference using torch.save. . Warning: This saves the entire module using Python&#8217;s pickle module. There are disadvantages to this approach. I used it here because it is more intuitive and is not the focus of this blog post, but feel free to read this official pytorch document to learn more about use cases regarding the saving and loading of Pytorch models. . The Fastai way . &quot;fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches.&quot;. . Read the docs to learn more! . Data preparation . block = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, batch_tfms=aug_transforms(mult=2., do_flip=False)) . The DataBlock class is a &quot;generic container to quickly build Datasets and DataLoaders&quot;. . blocks: This is the way of telling the API that our inputs are images and our targets are categories. Types are represented by blocks, here we use ImageBlock and CategoryBlock for inputs and targets respectively. . | get_items: expects a function to assemble our items inside the data block. get_image_files searches subfolder for all image filenames recursively. . | splitter: Controls how our validation set is created. RandomSplitter splits items between training and validation (with valid_pct portion in validation) randomnly. . | get_y: expects a function to label data according to file name. parent_label labels items with the parent folder name. . | batch_tfms: These are transformations applied to batched data samples on the GPU. aug_transforms is an &quot;utility function to create a list of flip, rotate, zoom, warp and lighting transforms&quot;. (Here we disabled flipping because we don&#39;t want to train on mirrored images and use twice the amount of augmentation compared to the default.) These augmentations are only done on the training set, we don&#39;t want to evaluate our model&#39;s performance on distorted images. . | . . Note: &quot;Our model can learn to focus on, and recognize, different features in our images. It also reflects how images work in the real world: different photos of the same thing may be framed in slightly different ways.In fact, an entirely untrained neural network knows nothing whatsoever about how images behave. It doesn&#8217;t even recognize that when an object is rotated by one degree, it still is a picture of the same thing! So actually training the neural network with examples of images where the objects are in slightly different places and slightly different sizes helps it to understand the basic concept of what an object is, and how it can be represented in an image.&quot; (Deep Learning for Coders with Fastai and PyTorch) . This doesn&#39;t acutally build the datasets and data loaders since we didn&#39;t actually gave it our images yet. But once we do, it knows exactly how to deal with them! . loaders = block.dataloaders(path/&quot;training&quot;) loaders.train.show_batch(max_n=4, nrows=1) . block.dataloaders creates a DataLoaders object from the source we give it. Here we gave it the training folder. The sample of images shown are outputs from the created training data loader. As you can see, they are correctly labeled and quite distorted due to the batch augmentations we made. . We use the default value of 64 for our batch size (bs parameter). . Training a neural network . learn = cnn_learner(loaders, resnet34, metrics=accuracy) . cnn_learner builds a convolutional neural network style learner from dataloaders and an architecture. In our case we use the ResNet architecture. The 34 refers to the number of layers in this variant of the architecture. . cnn_learner has a parameter called pretrained which defaults to True, that sets the weights in our model to values already trained by experts to recognize thousands of categories on the ImageNet dataset. . When using a pretrained model, cnn_learner will remove the last layer since that is always specifically customized to the original training task (i.e. ImageNet dataset classification), and replace it with one or more new layers with randomized weights (called the head), of an appropriate size for the dataset you are working with. . Tip: &quot;You should nearly always use a pretrained model, because it means that your model, before you&#8217;ve even shown it any of your data, is already very capable. And, as you&#8217;ll see, in a deep learning model many of these capabilities are things you&#8217;ll need, almost regardless of the details of your project. For instance, parts of pretrained models will handle edge, gradient, and color detection, which are needed for many tasks.&quot; (Deep Learning for Coders with Fastai and PyTorch) . learn.lr_find() . SuggestedLRs(lr_min=0.012022644281387329, lr_steep=0.007585775572806597) . learn.lr_find() explores learning rates in a given range ([1e-7, 10] by default) over a number of iterations (100 default) and plots the loss versus the learning rates on a log scale. . Tip: A rule of thumb is choosing a value that is approximately in the middle of the sharpest downward slope (around 1e-2 here). You can also use the lr_min and lr_steep indicators above and choose a learning rate between them. . learn.fine_tune(12, base_lr=1e-2, cbs=[ShowGraphCallback()]) . epoch train_loss valid_loss accuracy time . 0 | 0.720954 | 0.305432 | 0.901917 | 01:00 | . epoch train_loss valid_loss accuracy time . 0 | 0.206855 | 0.076552 | 0.978250 | 01:24 | . 1 | 0.154104 | 0.069363 | 0.978500 | 01:23 | . 2 | 0.122340 | 0.059505 | 0.983250 | 01:23 | . 3 | 0.110277 | 0.044111 | 0.987417 | 01:23 | . 4 | 0.091840 | 0.049989 | 0.985250 | 01:23 | . 5 | 0.074792 | 0.027301 | 0.992833 | 01:23 | . 6 | 0.071336 | 0.025267 | 0.992167 | 01:23 | . 7 | 0.065276 | 0.027973 | 0.991917 | 01:23 | . 8 | 0.043457 | 0.026573 | 0.991833 | 01:25 | . 9 | 0.041340 | 0.019284 | 0.994500 | 01:25 | . 10 | 0.033921 | 0.019462 | 0.994083 | 01:28 | . 11 | 0.030882 | 0.019053 | 0.994500 | 01:27 | . learn.fine_tune: &quot;Fine tune with freeze for freeze_epochs then with unfreeze for epochs using discriminative LR&quot; (docs) . By default a pretrained Learner is in a frozen state, meaning that only the head of the model will train while the body stays frozen. . To resume, fine_tune trains the head (automatically added by cnn_learner with random weights) without the body for a few epochs (defaults to 1) and then unfreezes the Learner and trains the whole model for a number of epochs (here we chose 12) using discriminative learning rates (which means it applies different learning rates for different parts of the model). . cbs expects a list of callbacks. Here we passed ShowGraphCallback which updates a graph of training and validation loss (as seen above). . Note: Discriminative learning rate is preferred on pretrained models since the early layers are already trained and already recognize features useful to our specific model. This means we don&#8217;t need to train these layers &quot;as hard&quot; (updating weights by smaller steps), which is why we use a range of learning rates from smaller ones for early layers to larger ones for later layers. . CONGRATS! . After training our model for a while, we get around 99.5% accuracy on our validation set with minimal effort! . learn.export(&quot;models/fastai-99acc.pkl&quot;) . learn.export saves the definition of how to create our DataLoaders on top of saving the architecture and parameters of the model. Saving the Dataloaders allows us to transform the data for inference in the same manner as our validation set by default, so data augmentation will not be applied. . interp = ClassificationInterpretation.from_learner(learn) . ClassificationInterpretation.from_learner() constructs an ClassificationInterpretation object from a learner. It gives a handful of interpretation methods for classification models. . interp.plot_confusion_matrix() . The above confusion matrix helps us visualize where our model made mistakes. It like the most confused number were 0 with 6, 6 with 8, 5 with 3 and 7 with 2. . interp.plot_top_losses(10) . We can also visualize which images resulted in the largest loss. It seems like the upper-left 9 was mislabeled as a 3, so our network was right. For some of those, even humans could have mistaken them ! . Evaluating our model&#39;s inference on the testing dataset! . Pytorch model . def test_model(model, criterion, test_loader): model = model.to(device) # Moves and/or casts the parameters and buffers to device. test_loss = 0.0 # keep track of loss test_corrects = 0 # count of carrectly classified inputs with torch.no_grad(): # Disable gradient calculation for inputs, labels in test_loader: inputs = inputs.to(device) # Perform Tensor device conversion labels = labels.to(device) outputs = model(inputs) # forward pass through network loss = criterion(outputs, labels) # Calculate loss _, preds = torch.max(outputs, 1) test_loss += loss * inputs.size(0) # multiply mean loss by the number of elements test_corrects += torch.sum(preds == labels.data) # add number of correct predictions to total avg_loss = test_loss / len(test_loader.dataset) # get the &quot;mean&quot; loss for the epoch avg_acc = test_corrects.float() / len(test_loader.dataset) # Get proportion of correct predictions return avg_loss.item(), avg_acc.item() . Our testing procedure is basically the same as our validation phase from the training procedure apart from the absence of epochs. (To be expected since they serve the same purpose!) . We infer predictions from our inputs by batches, then calculate the loss from them (how &quot;far&quot; they were from the real labels) and record the loss and the number of correctly labeled inputs, before averaging it all at the end. . testing_loader = torch.utils.data.DataLoader(testing_set, batch_size=bs) . Creation of a testing DataLoader to be passed to our testing procedure. . pytorch_loss, pytorch_accuracy = test_model(pytorch_net, criterion, testing_loader) . def print_loss_acc(loss, acc): print(&quot;Loss : {:.6f}&quot;.format(loss)) print(&quot;Accuracy : {:.6f}&quot;.format(acc)) . print_loss_acc(pytorch_loss, pytorch_accuracy) . Loss : 0.079889 Accuracy : 0.977300 . The results on the testing data are approximately the same as on the validation set! . Fastai model . learn = load_learner(&#39;models/fastai-99acc.pkl&#39;) . test_dl = learn.dls.test_dl(get_image_files(path/&quot;testing&quot;), with_labels=True) . test_dl creates a test dataloader from test_items (list of image paths) using validation transforms of dls. We set with_labels to True because we want the labels of each image to check the inference accuracy of our model. . fastai_loss, fastai_accuracy = learn.validate(dl=test_dl) . learn.validate returns the calculated loss and the metrics of the model on the dl data loader. . print_loss_acc(fastai_loss, fastai_accuracy) . Loss : 0.014902 Accuracy : 0.995400 . Our loss and accuracy are slightly better than on our validation set! . . Note: A better loss and accuracy on the testng dataset may suggest that the makers of the MNIST database chose more of the hard and ambiguous images for training in order to make the trained models more robust. . And that&#39;s about it! Not so hard heh! We now have a two digits classification models ready to be used for inference! . How was it ? . This is one of my first blog posts and it took me some time. Any feedback is welcome! . Was the whole model building process easier than expected? . Would you have done some parts a different way? . Was my explaination any good? . Please feel free to comment or annotate the text directly using Hypothes.is if you spotted any errors or have any questions! .",
            "url": "https://jonathan-sands.com/deep%20learning/fastai/pytorch/vision/classifier/2020/11/15/MNIST.html",
            "relUrl": "/deep%20learning/fastai/pytorch/vision/classifier/2020/11/15/MNIST.html",
            "date": " • Nov 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Building a CNN recognizing furniture from different Louis periods",
            "content": "Context . This project was done after reading chapter 2 of fastai&#39;s book. It aims at classifying furniture from 4 different eras : . Louis XIII | Louis XIV | Louis XV | Louis XVI | . The idea came from a friend of mine telling me there once was an endless debate at lunch about which era a particular piece of furniture came from. . Content . This notebook will take you through my building and deployment process using the fastai library. If you don&#39;t care about this and just want to check out the final result, please head here. . Building the dataset . To build the dataset, I used fatkun batch download image chrome plugin. There are many other ways to scrap the web for images, but in my case it turns out typing Louis XIV furniture (or should I say mobilier Louis XIV in french since this is a french thing) on Google image gives me a bunch of furniture from all eras. I found at that pinterest has some pretty good collections of images (for example here for Louis XIV styled furniture). It turns out this plugin makes it easy to go on a page then download every images from it, including trash (but you have the ability to remove it before downloading). I recommend you to go on every website/page you need to build your data before downloading, since the plugin detects duplicates (also avoiding the same trash images time and time again). . I put each type of images in it&#39;s own folder as you can see. . path = Path(&#39;data/train&#39;) path.ls() . (#4) [Path(&#39;data/train/XV&#39;),Path(&#39;data/train/XIII&#39;),Path(&#39;data/train/XVI&#39;),Path(&#39;data/train/XIV&#39;)] . Cleaning up . Let&#39;s remove corrupted images before moving on. . fns = get_image_files(path) # gets all the image files recursively in path failed = verify_images(fns) failed . █ . (#0) [] . As you can see, our dataset doesn&#39;t have any, and I am not sure if it can happen while using this plugin. But if you ever do have some, execute the next step. . failed.map(Path.unlink) . (#0) [] . From Data to DataLoaders . You should really go an read the chapter from the book if you want to understand what I am doing. I am basically, copy pasting their stuff, even this title is the same ! . furniture = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=1), # Fixed seed for tuning hyperparameters get_y=parent_label, item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = furniture.dataloaders(path) . Let&#39;s take a look at some of does images ! . dls.valid.show_batch(max_n=4, nrows=1) . Okay, looking good ! But we have lots of data. . len(fns) . 2052 . And among this data are probably some mislabeled ones and some that aren&#39;t even furniture ! To help us out in our cleaning process, we will use a quickly trained model. . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time 0 1.752790 0.920795 0.319095 00:06 epoch train_loss valid_loss error_rate time 0 1.079975 0.690126 0.258794 00:07 1 0.904969 0.654678 0.211055 00:07 2 0.726501 0.621098 0.208543 00:07 3 0.610325 0.598395 0.203518 00:07 . Even if we quickly trained our model on a small architecture (resnet18), we can get an idea of what the model has trouble identifying by plotting a confusion matrix. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . █ . But we haven&#39;t cleaned anything yet so let&#39;s do that ! . cleaner = ImageClassifierCleaner(learn) cleaner . █ . for idx in cleaner.delete(): cleaner.fns[idx].unlink() # Delete files you selected as Delete . for idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat) # Change label from mislabeled images . I redid this whole procedure another time, just to be sure. And I probably should do it again. Unfortunately, I am no furniture expert and can&#39;t really relabel mislabeled data so I guess my dataset is full of it. . Training the actual model ! . Now that our dataset is clean (not really haha), we can start training our actual production model ! . path = Path(&#39;data/train&#39;) fns = get_image_files(path) len(fns) . 1992 . Our dataset is smaller now that we removed 60 images ! . furniture = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=1), # Fixed seed for tuning hyperparameters get_y=parent_label, item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) . dls = furniture.dataloaders(path,bs=32) . learn = cnn_learner(dls, resnet50, metrics=error_rate) learn.fine_tune(8, cbs=[ShowGraphCallback()]) . epoch train_loss valid_loss error_rate time . 0 | 1.424473 | 1.235989 | 0.351759 | 00:12 | . epoch train_loss valid_loss error_rate time . 0 | 0.890016 | 0.578133 | 0.193467 | 00:16 | . 1 | 0.728691 | 0.681202 | 0.178392 | 00:16 | . 2 | 0.672835 | 0.601211 | 0.180905 | 00:16 | . 3 | 0.510261 | 0.606285 | 0.175879 | 00:16 | . 4 | 0.341562 | 0.585303 | 0.160804 | 00:16 | . 5 | 0.211855 | 0.521299 | 0.153266 | 00:16 | . 6 | 0.148523 | 0.524159 | 0.135678 | 00:16 | . 7 | 0.126175 | 0.551581 | 0.135678 | 00:16 | . learn.save(&#39;resnet50-13_error_rate&#39;) . Path(&#39;models/resnet50-13_error_rate.pth&#39;) . We used transfer learning, using resnet50 as our pretrained model (resnet50 architecture with pretrained weights from the ImageNet dataset) and a batch size of 32 (since I am training on my personal GPU RTX2060 that couldn&#39;t handle a larger batch size). As you can see, our error_rate on the validation set is aroud 13%, which is not great, but considering our approximate dataset, is understandable. . Making a web app . We are going now to make a notebook app and deploy it. . First let us save our learn object. . learn.export() . path = Path() path.ls(file_exts = &#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . learn_inf = load_learner(path/&#39;export.pkl&#39;) . To make our app, we will use IPython widgets and Voilà. . btn_upload = widgets.FileUpload() out_pl = widgets.Output() lbl_pred = widgets.Label() btn_run = widgets.Button(description=&#39;Classify&#39;) . Let&#39;s define what uploading an image does. . def on_data_change(change): lbl_pred.value = &#39;&#39; img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . btn_upload.observe(on_data_change, names=[&#39;data&#39;]) . display(VBox([widgets.Label(&#39;Select your piece of Louis furniture !&#39;), btn_upload, out_pl, lbl_pred])) . Above is what the user sees when he launches our app. . When uploading an image, the whole thing looks like this ! . Now, we just have to deploy our app ! Voilà can transform you jupyter notebook into a simple working web app. Of course we aren&#39;t transforming this entire notebook but instead this one here. . We are deploying our app on Binder. Check out these documentations on how to use voilà with Binder. . And TADAAAA! That&#39;s all it took ! Check out the working web app . Conclusion . What do I take out of this small project ? . Things that worked great . training | building app | deploying app | . Things that didn&#39;t work great / were hard . getting good data (lots of mislabeled data) | cleaning it up (lots of out-of-domain data) | finding the right number of epoch felt a little random | . Things to look into: . Find a way to get cleaner data | Perhaps use a different slice than default for fine tuning | Make the app cleaner | .",
            "url": "https://jonathan-sands.com/jupyter/fastai/voila/cnn/deep%20learning/2020/09/23/Louis_CNN.html",
            "relUrl": "/jupyter/fastai/voila/cnn/deep%20learning/2020/09/23/Louis_CNN.html",
            "date": " • Sep 23, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About this blog",
          "content": "Why ? . I created this blog after reading chapter 2 of fastai’s book, teaching deep learning with a top down approach, where they recommended starting a blog. . . I really recommend checking their tutorials and courses, they are great I don’t expect any clear content to start with. This is also a way to help me learn and get things sorted out inside my head. But hey, if you are reading this, you may be in the same situation and can get to learn along with me :wink:. . Content to expect . This will be a list of what I think will appear on this blog in the future, based on what I am currently doing / learning and what I want to learn in the future. . Machine learning Deep learning | deep reinforcement learning | generative modeling | others | . | Some maths1 | Rambling about topics such as: Rocket Science and other science mystery things (sounds scary) | AI | Engineering and technology stuff | . | . I recently ordered my first math book and I intend to learn and understand math more deeply &#8617; . |",
          "url": "https://jonathan-sands.com/about-blog/",
          "relUrl": "/about-blog/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "About Me",
          "content": ". Important: Download CV Hi ! My name is Jonathan SANDS, a french engineering student just starting a master’s program in AI. If you want to learn more about me then you are in the right place ! . . Note: If you want to know more about the content of this blog, please head here instead ! :wink: Quick overview of myself . 2016: French Baccalauréat, France’s national highschool exam. (17.9/20 average in Science, 15.5/20 average accounting all subjects) . | 2016-2019: Studied at EPITA, a french computer science and software engineering focused engineering school. . | 2018: I spent the first semester abroad in Beijing at BJTU, as part of an exchange agreement with my french school. . | 2019-now: Studying at ESILV, another french engineering school part of the “Pôle Universitaire Léonard de Vinci” (a group of several schools). I did my last year of undergraduate here, and just joined its master’s degree program in AI and data science. . | . . I transfered from EPITA to ESILV because EPITA was mostly about coding brainlessly, whereas ESILV has a more scientific based approach I like better 2020: During the first semester, I left for Romania to study at UBB, as part of an exchange program with my french school. | . . I was supposed to study a master&#39;s program in Control Science and Engineering at SJTU in Shanghai, as part of the UM-SJTU joint institute, starting 2020 for 2.5 years but I changed my mind because of the COVID-19 virus :cry: Interests . Techie / Science stuff Computer science | Rocket Science | Math | Everything new and exciting ! | . | Life stuff Travelling abroad / experiencing new cultures | Sports (volleyball, calisthenics, climbing, running, anything really :grin:) | Any new experience ! | . | . Medias . . Tip: Click on the logos to get to my various profiles ! . . . . . . There was a &#39;Forbidden&#39; error fetching URL: &#39;https://twitter.com/JonathanSands14/status/1309793203407802370&#39; . . . . Don&#39;t expect to see much here",
          "url": "https://jonathan-sands.com/about-me/",
          "relUrl": "/about-me/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jonathan-sands.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}