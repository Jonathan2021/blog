{
  
    
        "post0": {
            "title": "Building a CNN recognizing furniture from different Louis periods",
            "content": "Context . This project was done after reading chapter 2 of fastai&#39;s book. It aims at classifying furniture from 3 different eras : . Louis XIV | Louis XV | Louis XVI | . The idea came from a friend of mine telling me there once was an endless debate at lunch about which era a particular piece of furniture came from. . Content . This notebook will take you through my building and deployment process using the fastai library. If you don&#39;t care about this and just want to check out the final result, please head here. . Building the dataset . To build the dataset, I used fatkun batch download image chrome plugin. There are many other ways to scrap the web for images, but in my case it turns out typing Louis XIV furniture (or should I say mobilier Louis XIV in french since this is a french thing) on Google image gives me a bunch of furniture from all eras. I found at that pinterest has some pretty good collections of images (for example here for Louis XIV styled furniture). It turns out this plugin makes it easy to go on a page then download every images from it, including trash (but you have the ability to remove it before downloading). I recommend you to go on every website/page you need to build your data before downloading, since the plugin detects duplicates (also avoiding the trash images time and time again). . I put each type of images in it&#39;s own folder as you can see. . path = Path(&#39;data/train&#39;) path.ls() . (#4) [Path(&#39;data/train/XV&#39;),Path(&#39;data/train/XIII&#39;),Path(&#39;data/train/XVI&#39;),Path(&#39;data/train/XIV&#39;)] . Cleaning up . Let&#39;s remove corrupted images before moving on. . fns = get_image_files(path) # gets all the image files recursively in path failed = verify_images(fns) failed . █ . (#0) [] . As you can see, our dataset doesn&#39;t have any, and I am not sure if it can happen while using this plugin. But if you ever do have some, execute the next step. . failed.map(Path.unlink) . (#0) [] . From Data to DataLoaders . You should really go an read the chapter from the book if you want to understand what I am doing. I am basically, copy pasting their stuff, even this title is the same ! . furniture = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=1), # Fixed seed for tuning hyperparameters get_y=parent_label, item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = furniture.dataloaders(path) . Let&#39;s take a look at some of does images ! . dls.valid.show_batch(max_n=4, nrows=1) . Okay, looking good ! But we have lots of data. . len(fns) . 2052 . And among this data are probably some mislabeled ones and some that aren&#39;t even furniture ! To help us out in our cleaning process, we will use a quickly trained model. . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time 0 1.752790 0.920795 0.319095 00:06 epoch train_loss valid_loss error_rate time 0 1.079975 0.690126 0.258794 00:07 1 0.904969 0.654678 0.211055 00:07 2 0.726501 0.621098 0.208543 00:07 3 0.610325 0.598395 0.203518 00:07 . Even if we quickly trained our model on a small architecture (resnet18), we can get an idea of what the model has trouble identifying by plotting a confusion matrix. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . █ . But we haven&#39;t cleaned anything yet so let&#39;s do that ! . cleaner = ImageClassifierCleaner(learn) cleaner . █ . for idx in cleaner.delete(): cleaner.fns[idx].unlink() # Delete files you selected as Delete . for idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat) # Change label from mislabeled images . I redid this whole procedure another time, just to be sure. And I probably should do it again. Unfortunately, I am no furniture expert and can&#39;t really relabel mislabeled data so I guess my dataset is full of it. . Training the actual model ! . Now that our dataset is clean (not really haha), we can start training our actual production model ! . path = Path(&#39;data/train&#39;) fns = get_image_files(path) len(fns) . 1992 . Our dataset is smaller now that we removed 60 images ! . furniture = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=1), # Fixed seed for tuning hyperparameters get_y=parent_label, item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) . dls = furniture.dataloaders(path,bs=32) . learn = cnn_learner(dls, resnet50, metrics=error_rate) learn.fine_tune(8, cbs=[ShowGraphCallback()]) . epoch train_loss valid_loss error_rate time . 0 | 1.424473 | 1.235989 | 0.351759 | 00:12 | . epoch train_loss valid_loss error_rate time . 0 | 0.890016 | 0.578133 | 0.193467 | 00:16 | . 1 | 0.728691 | 0.681202 | 0.178392 | 00:16 | . 2 | 0.672835 | 0.601211 | 0.180905 | 00:16 | . 3 | 0.510261 | 0.606285 | 0.175879 | 00:16 | . 4 | 0.341562 | 0.585303 | 0.160804 | 00:16 | . 5 | 0.211855 | 0.521299 | 0.153266 | 00:16 | . 6 | 0.148523 | 0.524159 | 0.135678 | 00:16 | . 7 | 0.126175 | 0.551581 | 0.135678 | 00:16 | . learn.save(&#39;resnet50-13_error_rate&#39;) . Path(&#39;models/resnet50-13_error_rate.pth&#39;) . We used transfer learning, using resnet50 as our pretrained model (resnet50 architecture with pretrained weights from the ImageNet dataset) and a batch size of 32 (since I am training on my personal GPU RTX2060 that couldn&#39;t handle a larger batch size). As you can see, our error_rate on the validation set is aroud 13%, which is not great, but considering our approximate dataset, is understandable. . Making a web app . We are going now to make a notebook app and deploy it. . First let us save our learn object. . learn.export() . path = Path() path.ls(file_exts = &#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . learn_inf = load_learner(path/&#39;export.pkl&#39;) . To make our app, we will use IPython widgets and Voilà. . btn_upload = widgets.FileUpload() out_pl = widgets.Output() lbl_pred = widgets.Label() btn_run = widgets.Button(description=&#39;Classify&#39;) . Let&#39;s define what uploading an image does. . def on_data_change(change): lbl_pred.value = &#39;&#39; img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . btn_upload.observe(on_data_change, names=[&#39;data&#39;]) . display(VBox([widgets.Label(&#39;Select your piece of Louis furniture !&#39;), btn_upload, out_pl, lbl_pred])) . Above is what the user sees when he launches our app. . When uploading an image, the whole thing looks like this ! . Now, we just have to deploy our app ! Voilà can transform you jupyter notebook into a simple working web app. Of course we aren&#39;t transforming this entire notebook but instead this one here. . We are deploying our app on Binder. Check out these documentations on how to use voilà with Binder. . And TADAAAA! That&#39;s all it took ! Check out the working web app . Conclusion . What do I take out of this small project ? . Things that worked great . training | building app | deploying app | . Things that didn&#39;t work great / were hard . getting good data (lots of mislabeled data) | cleaning it up (lots of out-of-domain data) | finding the right number of epoch felt a little random | . Things to look into: . Find a way to get cleaner data | Perhaps use a different slice than default for fine tuning | Make the app cleaner | .",
            "url": "https://jonathan-sands.com/jupyter/fastai/voila/cnn/deep%20learning/2020/09/23/Louis_CNN.html",
            "relUrl": "/jupyter/fastai/voila/cnn/deep%20learning/2020/09/23/Louis_CNN.html",
            "date": " • Sep 23, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About this blog",
          "content": "Why ? . I created this blog after reading chapter 2 of fastai’s book, teaching deep learning with a top down approach, where they recommended starting a blog. . . I really recommend checking their tutorials and courses, they are great I don’t expect any clear content to start with. This is also a way to help me learn and get things sorted out inside my head. But hey, if you are reading this, you may be in the same situation and can get to learn along with me :wink:. . Content to expect . This will be a list of what I think will appear on this blog in the future, based on what I am currently doing / learning and what I want to learn in the future. . Machine learning Deep learning | deep reinforcement learning | generative modeling | others | . | Some maths1 | Rambling about topics such as: Rocket Science and other science mystery things (sounds scary) | AI | Engineering and technology stuff | . | . I recently ordered my first math book and I intend to learn and understand math more deeply &#8617; . |",
          "url": "https://jonathan-sands.com/about-blog/",
          "relUrl": "/about-blog/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "About Me",
          "content": ". Important: Download CV Hi ! My name is Jonathan SANDS, a french engineering student just starting a master’s program in AI. If you want to learn more about me then you are in the right place ! . . Note: If you want to know more about the content of this blog, please head here instead ! :wink: Quick overview of myself . 2016: French Baccalauréat, France’s national highschool exam. (17.9/20 average in Science, 15.5/20 average accounting all subjects) . | 2016-2019: Studied at EPITA, a french computer science and software engineering focused engineering school. . | 2018: I spent the first semester abroad in Beijing at BJTU, as part of an exchange agreement with my french school. . | 2019-now: Studying at ESILV, another french engineering school part of the “Pôle Universitaire Léonard de Vinci” (a group of several schools). I did my last year of undergraduate here, and just joined its master’s degree program in AI and data science. . | . . I transfered from EPITA to ESILV because EPITA was mostly about coding brainlessly, whereas ESILV has a more scientific based approach I like better 2020: During the first semester, I left for Romania to study at UBB, as part of an exchange program with my french school. | . . I was supposed to study a master&#39;s program in Control Science and Engineering at SJTU in Shanghai, as part of the UM-SJTU joint institute, starting 2020 for 2.5 years but I changed my mind because of the COVID-19 virus :cry: Interests . Techie / Science stuff Computer science | Rocket Science | Math | Everything new and exciting ! | . | Life stuff Travelling abroad / experiencing new cultures | Sports (volleyball, calisthenics, climbing, running, anything really :grin:) | Any new experience ! | . | . Medias . . Tip: Click on the logos to get to my various profiles ! . . . . . . Just created my #blog to share my learning experience and projects with others !https://t.co/xFX6TLOQdr . &mdash; Jonathan Sands (@JonathanSands14) September 26, 2020 . . . Don&#39;t expect to see much here",
          "url": "https://jonathan-sands.com/about-me/",
          "relUrl": "/about-me/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jonathan-sands.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}